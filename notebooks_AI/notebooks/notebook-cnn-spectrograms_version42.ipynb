{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee428bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T09:44:39.819966Z",
     "iopub.status.busy": "2025-02-26T09:44:39.819764Z",
     "iopub.status.idle": "2025-02-26T09:44:39.826232Z",
     "shell.execute_reply": "2025-02-26T09:44:39.825619Z"
    },
    "papermill": {
     "duration": 0.015495,
     "end_time": "2025-02-26T09:44:39.827768",
     "exception": false,
     "start_time": "2025-02-26T09:44:39.812273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PJRT_DEVICE'] = 'TPU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deac1fe6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-26T09:44:39.840407Z",
     "iopub.status.busy": "2025-02-26T09:44:39.840214Z",
     "iopub.status.idle": "2025-02-26T09:45:01.109178Z",
     "shell.execute_reply": "2025-02-26T09:45:01.108317Z"
    },
    "papermill": {
     "duration": 21.277291,
     "end_time": "2025-02-26T09:45:01.110916",
     "exception": false,
     "start_time": "2025-02-26T09:44:39.833625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mynumpyfiles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mynumpyfiles/NumpyDS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mynumpyfiles/NumpyDS/test_Npy_fold_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mynumpyfiles/NumpyDS/train_Npy_fold_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mynumpyfiles/NumpyDS/train_Npy_fold_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mynumpyfiles/NumpyDS/train_Npy_fold_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mynumpyfiles/NumpyDS/test_Npy_fold_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mynumpyfiles/NumpyDS/test_Npy_fold_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mynumpyfiles/NumpyDS/train_Npy_fold_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mynumpyfiles/NumpyDS/test_Npy_fold_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mynumpyfiles/NumpyDS/test_Npy_fold_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mynumpyfiles/NumpyDS/train_Npy_fold_3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import csv\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    print(dirname)\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf326d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T09:45:01.126015Z",
     "iopub.status.busy": "2025-02-26T09:45:01.125619Z",
     "iopub.status.idle": "2025-02-26T09:46:06.524633Z",
     "shell.execute_reply": "2025-02-26T09:46:06.522443Z"
    },
    "papermill": {
     "duration": 65.408324,
     "end_time": "2025-02-26T09:46:06.526090",
     "exception": false,
     "start_time": "2025-02-26T09:45:01.117766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (23.0.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.8 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m1.7/1.8 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: pip\r\n",
      "  Attempting uninstall: pip\r\n",
      "    Found existing installation: pip 23.0.1\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling pip-23.0.1:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled pip-23.0.1\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed pip-25.0.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.18.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling tensorflow-2.18.0:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully uninstalled tensorflow-2.18.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-cpu\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading tensorflow_cpu-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.1.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (25.1.24)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (18.1.1)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.4.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (24.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (5.29.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (75.8.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.17.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.17.2)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.70.0)\r\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.18.0)\r\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.8.0)\r\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.0.2)\r\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.12.1)\r\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.4.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.37.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-cpu) (0.45.1)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow-cpu) (13.9.4)\r\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow-cpu) (0.0.8)\r\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow-cpu) (0.14.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2025.1.31)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu) (3.7)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu) (3.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-cpu) (3.0.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow-cpu) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow-cpu) (2.19.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-cpu) (0.1.2)\r\n",
      "Downloading tensorflow_cpu-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (230.0 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/230.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/230.0 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/230.0 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/230.0 MB\u001b[0m \u001b[31m160.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/230.0 MB\u001b[0m \u001b[31m163.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m159.6/230.0 MB\u001b[0m \u001b[31m162.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m192.9/230.0 MB\u001b[0m \u001b[31m172.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m204.7/230.0 MB\u001b[0m \u001b[31m147.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m229.9/230.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m229.9/230.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m229.9/230.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m229.9/230.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m229.9/230.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m229.9/230.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m229.9/230.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m229.9/230.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m229.9/230.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m229.9/230.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m229.9/230.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m229.9/230.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.0/230.0 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tensorflow-cpu\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed tensorflow-cpu-2.18.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip uninstall -y tensorflow && pip install tensorflow-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3db18d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T09:46:06.549120Z",
     "iopub.status.busy": "2025-02-26T09:46:06.548703Z",
     "iopub.status.idle": "2025-02-26T09:46:40.529363Z",
     "shell.execute_reply": "2025-02-26T09:46:40.528253Z"
    },
    "papermill": {
     "duration": 33.995979,
     "end_time": "2025-02-26T09:46:40.532466",
     "exception": false,
     "start_time": "2025-02-26T09:46:06.536487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "import torch_xla.runtime as xr\n",
    "\n",
    "from torch_xla.distributed.parallel_loader import MpDeviceLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec59e338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T09:46:40.569629Z",
     "iopub.status.busy": "2025-02-26T09:46:40.569142Z",
     "iopub.status.idle": "2025-02-26T09:46:40.649637Z",
     "shell.execute_reply": "2025-02-26T09:46:40.648783Z"
    },
    "papermill": {
     "duration": 0.101814,
     "end_time": "2025-02-26T09:46:40.652022",
     "exception": false,
     "start_time": "2025-02-26T09:46:40.550208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "myMeta=\"/kaggle/input/mynumpyfiles/NumpyDS/GlobalFoldsFiles_Train_Test_Augmented_Cleaned.csv\"\n",
    "myGlobalDs=pd.read_csv(myMeta)\n",
    "myGlobalDs.drop(myGlobalDs.columns[myGlobalDs.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "myGlobalDs[\"FilePath\"] = [fpath.replace(\"\\\\\", \"/\") for fpath in myGlobalDs[\"FilePath\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05ce6fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T09:46:40.680632Z",
     "iopub.status.busy": "2025-02-26T09:46:40.680314Z",
     "iopub.status.idle": "2025-02-26T09:46:40.703182Z",
     "shell.execute_reply": "2025-02-26T09:46:40.701990Z"
    },
    "papermill": {
     "duration": 0.04024,
     "end_time": "2025-02-26T09:46:40.704773",
     "exception": false,
     "start_time": "2025-02-26T09:46:40.664533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Type</th>\n",
       "      <th>nb</th>\n",
       "      <th>Label</th>\n",
       "      <th>condLabel</th>\n",
       "      <th>FileName</th>\n",
       "      <th>FilePath</th>\n",
       "      <th>LabelIDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10_T02 M06 (VHM 309-12)</td>\n",
       "      <td>VHM</td>\n",
       "      <td>0006_label_T02 M06 (VHM 309-12).npy</td>\n",
       "      <td>test_Npy_fold_0/0006_label_T02 M06 (VHM 309-12...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10_T02 M06 (VHM 309-12)</td>\n",
       "      <td>VHM</td>\n",
       "      <td>0022_label_T02 M06 (VHM 309-12).npy</td>\n",
       "      <td>test_Npy_fold_0/0022_label_T02 M06 (VHM 309-12...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10_T02 M06 (VHM 309-12)</td>\n",
       "      <td>VHM</td>\n",
       "      <td>0028_label_T02 M06 (VHM 309-12).npy</td>\n",
       "      <td>test_Npy_fold_0/0028_label_T02 M06 (VHM 309-12...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>29.0</td>\n",
       "      <td>10_T02 M06 (VHM 309-12)</td>\n",
       "      <td>VHM</td>\n",
       "      <td>0030_label_T02 M06 (VHM 309-12).npy</td>\n",
       "      <td>test_Npy_fold_0/0030_label_T02 M06 (VHM 309-12...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>33.0</td>\n",
       "      <td>10_T02 M06 (VHM 309-12)</td>\n",
       "      <td>VHM</td>\n",
       "      <td>0034_label_T02 M06 (VHM 309-12).npy</td>\n",
       "      <td>test_Npy_fold_0/0034_label_T02 M06 (VHM 309-12...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold  Type    nb                    Label condLabel  \\\n",
       "0     0  Test   5.0  10_T02 M06 (VHM 309-12)       VHM   \n",
       "1     0  Test  21.0  10_T02 M06 (VHM 309-12)       VHM   \n",
       "2     0  Test  27.0  10_T02 M06 (VHM 309-12)       VHM   \n",
       "3     0  Test  29.0  10_T02 M06 (VHM 309-12)       VHM   \n",
       "4     0  Test  33.0  10_T02 M06 (VHM 309-12)       VHM   \n",
       "\n",
       "                              FileName  \\\n",
       "0  0006_label_T02 M06 (VHM 309-12).npy   \n",
       "1  0022_label_T02 M06 (VHM 309-12).npy   \n",
       "2  0028_label_T02 M06 (VHM 309-12).npy   \n",
       "3  0030_label_T02 M06 (VHM 309-12).npy   \n",
       "4  0034_label_T02 M06 (VHM 309-12).npy   \n",
       "\n",
       "                                            FilePath  LabelIDs  \n",
       "0  test_Npy_fold_0/0006_label_T02 M06 (VHM 309-12...         0  \n",
       "1  test_Npy_fold_0/0022_label_T02 M06 (VHM 309-12...         0  \n",
       "2  test_Npy_fold_0/0028_label_T02 M06 (VHM 309-12...         0  \n",
       "3  test_Npy_fold_0/0030_label_T02 M06 (VHM 309-12...         0  \n",
       "4  test_Npy_fold_0/0034_label_T02 M06 (VHM 309-12...         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myGlobalDs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "729488ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T09:46:40.723991Z",
     "iopub.status.busy": "2025-02-26T09:46:40.723754Z",
     "iopub.status.idle": "2025-02-26T09:46:40.737973Z",
     "shell.execute_reply": "2025-02-26T09:46:40.737126Z"
    },
    "papermill": {
     "duration": 0.026073,
     "end_time": "2025-02-26T09:46:40.739778",
     "exception": false,
     "start_time": "2025-02-26T09:46:40.713705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>nb</th>\n",
       "      <th>LabelIDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11780.000000</td>\n",
       "      <td>7215.000000</td>\n",
       "      <td>11780.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>991.312266</td>\n",
       "      <td>4.573005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.414274</td>\n",
       "      <td>533.009652</td>\n",
       "      <td>3.262599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1010.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1477.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1837.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Fold           nb      LabelIDs\n",
       "count  11780.000000  7215.000000  11780.000000\n",
       "mean       2.000000   991.312266      4.573005\n",
       "std        1.414274   533.009652      3.262599\n",
       "min        0.000000     5.000000      0.000000\n",
       "25%        1.000000   515.000000      2.000000\n",
       "50%        2.000000  1010.000000      5.000000\n",
       "75%        3.000000  1477.000000      7.000000\n",
       "max        4.000000  1837.000000     10.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myGlobalDs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a205ea85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T09:46:40.759316Z",
     "iopub.status.busy": "2025-02-26T09:46:40.759096Z",
     "iopub.status.idle": "2025-02-26T09:46:44.743294Z",
     "shell.execute_reply": "2025-02-26T09:46:44.741966Z"
    },
    "papermill": {
     "duration": 3.996925,
     "end_time": "2025-02-26T09:46:44.745499",
     "exception": false,
     "start_time": "2025-02-26T09:46:40.748574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "\n",
    "import math, random\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DataUtils():\n",
    "    #---------------------------------------------\n",
    "    #Load a file and returns it as a list\n",
    "    #---------------------------------------------\n",
    "    @staticmethod\n",
    "    def x_loading(subsetXTrainFiles, myDir):\n",
    "        #Takes a list of files and returns a numpy array of shape (nObservations, nChannels, nSamples)\n",
    "        X_subset=[]\n",
    "        \n",
    "        for file in subsetXTrainFiles:    \n",
    "            myArray=np.load(os.path.join(myDir, file))\n",
    "            X_subset.append(myArray)\n",
    "        X_subset=np.array(X_subset) #X_subset.shape (1989 observations, 4 channels :sound, accel X,Y,Z, 250062 samples per observation)\n",
    "        \n",
    "        return X_subset\n",
    "    \n",
    "    @staticmethod\n",
    "    def saveFile(myNpy, destDir, fileName):\n",
    "        \"\"\"Save a file.\"\"\"\n",
    "        np.save(os.path.join(destDir, fileName), myNpy)\n",
    "\n",
    "    #---------------------------------------------\n",
    "    #Align the data to have a fixed duration.\n",
    "    #---------------------------------------------\n",
    "    @staticmethod\n",
    "    def padTruncate(mySig, maxTime, fs):\n",
    "        \"\"\"Pad or truncate the data to have a fixed duration.\n",
    "        Args:\n",
    "            mySig: the signal as a list\n",
    "            maxTime: The maximum time in milliseconds.\n",
    "            fs: the sampling rate in Hz    \n",
    "            \"\"\"\n",
    "        sLen = mySig.shape[0]\n",
    "        trunc=0\n",
    "        maxLen=int(maxTime*int(fs/1000))\n",
    "        if sLen > maxLen:\n",
    "            trunc=1\n",
    "            mySig = mySig[:, :maxLen]\n",
    "        elif sLen < maxLen:\n",
    "            trunc=-1\n",
    "            mySig = np.pad(mySig, ((0, int(maxLen - sLen)),(0,0)), mode='wrap')\n",
    "        \n",
    "        return (trunc, len(mySig), mySig)\n",
    "    \n",
    "    def addGaussianNoise(mySig, std=0.02):\n",
    "        \"\"\"Add a gaussian noise.\"\"\"\n",
    "        mySig_float=mySig.astype(np.float32)\n",
    "        noise = np.random.normal(0, std*np.std(np.abs(mySig_float), axis=0), mySig.shape)\n",
    "        noisySig=np.clip(mySig_float+noise, -32768,32767)        \n",
    "        return noisySig.astype(np.int16)\n",
    "    \n",
    "    def timeShift(mySig, shift_lim=0.5):\n",
    "        \"\"\"Shifts the data in time (reintroduce ending values at the beginning.\"\"\"\n",
    "        _, sLen = mySig.shape\n",
    "        shiftAmt = int(np.random.random() * shift_lim * sLen)\n",
    "        return np.roll(mySig, shiftAmt, axis=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def augment_fft(data, factor=1.2):\n",
    "        \"\"\"Augmente ou diminue certaines fréquences du signal\"\"\"\n",
    "        \n",
    "        fft_data = np.fft.fft(data, axis=0)\n",
    "        fft_data = fft_data * factor  # Modification du spectre\n",
    "        newData=np.real(np.fft.ifft(fft_data, axis=0))  # Retourne le signal temporel\n",
    "        newData = np.clip(newData, -32768, 32767)\n",
    "        return newData.astype(np.int16)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Generate a Mel Spectrogram Excellent for Audio Data\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def mel_spectro_gram(mySig, n_mels=64, n_fft=1024, hop_len=None, fs=50000):\n",
    "        if hop_len is None:\n",
    "            hop_len = n_fft // 4\n",
    "            \n",
    "        top_db = 80\n",
    "        mySig=mySig.astype(np.float32)\n",
    "        mySigTensor=torch.from_numpy(mySig.T)\n",
    "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "        spec = transforms.MelSpectrogram(fs, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(mySigTensor)\n",
    "        # Convert to decibels\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        spec = spec.unsqueeze(0) if spec.dim() == 2 else spec\n",
    "        return (spec, hop_len)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Generate a \"classic\" Spectrogram for Accelerometer Data\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_gram(mySig, n_fft=1024, hop_len=None, fs=50000):\n",
    "        if hop_len is None:\n",
    "            hop_len = n_fft // 4\n",
    "            \n",
    "        mySig=mySig.astype(np.float32)\n",
    "        mySigTensor=torch.from_numpy(mySig.T)\n",
    "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "        spec = transforms.Spectrogram(n_fft=n_fft, hop_length=hop_len, power=2)(mySigTensor)\n",
    "        # Convert to decibels\n",
    "        spec = transforms.AmplitudeToDB()(spec)\n",
    "        spec = spec.unsqueeze(0) if spec.dim() == 2 else spec\n",
    "        return (spec, hop_len)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Visualize a Spectrogram\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def show_spectrogram(spec, fs=50000, hop=256, title=\"Spectrogram\", ylabel='Mel Frequency Bands', channels=['Sound', 'AccX', 'AccY', 'AccZ'], mel=False):\n",
    "        nbChannels=spec.shape[0]\n",
    "        fig, axs = plt.subplots(nbChannels, 1, figsize=(18, 12))\n",
    "        if nbChannels==1:\n",
    "            axs=[axs]\n",
    "        timeAxis=np.arange(0, spec.shape[2]*hop, hop)/fs\n",
    "        freqAxis = np.linspace(0, fs / 2, spec.shape[1])\n",
    "        for i in range(nbChannels):\n",
    "            if mel:\n",
    "                img=axs[i].imshow(spec[i].detach().numpy(), aspect='auto', origin='lower', cmap='viridis', extent=[timeAxis[0], timeAxis[-1], 0, spec[i].shape[0]])\n",
    "            else:\n",
    "                img=axs[i].imshow(spec[i].detach().numpy(), aspect='auto', origin='lower', cmap='viridis', extent=[timeAxis[0], timeAxis[-1], freqAxis[0], freqAxis[-1]])\n",
    "                \n",
    "            axs[i].set_title(f\"Channel {channels[i]}\")\n",
    "            axs[i].set_ylabel(ylabel)\n",
    "        plt.xlabel('Time [s]')\n",
    "        plt.suptitle(title)\n",
    "        plt.colorbar(img, ax=axs, orientation='vertical')\n",
    "        plt.show()\n",
    "        \n",
    "    # ----------------------------\n",
    "    # Augment a Spectrogram\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_Augment(spectro, maxMaskPercentage=0.1,nFreqMask=1, nTimeMasks=1):\n",
    "        \"\"\"Augmente ou diminue certaines fréquences du signal\"\"\"\n",
    "        nChannel, nFreq, nSteps = spectro.shape\n",
    "        mask_value=spectro.mean()\n",
    "        aug_spec=spectro.clone()\n",
    "        freqMaskParam=maxMaskPercentage*nFreq\n",
    "        timeMaskParam=maxMaskPercentage*nSteps\n",
    "        \n",
    "        for ch in range(nChannel):\n",
    "            channelSpec=aug_spec[ch]\n",
    "            for _ in range(nFreqMask):\n",
    "                channelSpec=transforms.FrequencyMasking(freqMaskParam)(channelSpec, mask_value)\n",
    "\n",
    "            for _ in range(nTimeMasks):\n",
    "                channelSpec=transforms.TimeMasking(timeMaskParam)(channelSpec, mask_value)\n",
    "            aug_spec[ch]=channelSpec\n",
    "            \n",
    "        return aug_spec\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f75b73f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T09:46:44.766910Z",
     "iopub.status.busy": "2025-02-26T09:46:44.766374Z",
     "iopub.status.idle": "2025-02-26T09:46:44.774266Z",
     "shell.execute_reply": "2025-02-26T09:46:44.773078Z"
    },
    "papermill": {
     "duration": 0.020925,
     "end_time": "2025-02-26T09:46:44.775880",
     "exception": false,
     "start_time": "2025-02-26T09:46:44.754955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#With TPU\n",
    "from torch_xla.distributed.parallel_loader import MpDeviceLoader\n",
    "\n",
    "\n",
    "#Creation of a custom Dataset for PyTorch\n",
    "\n",
    "#df must contain Label and filepath only for one fold of training data. Folds will be dealt with externally to the DataLoader class.\n",
    "\n",
    "#Datapath is the path to the folder containing all the folds (i.e. F:\\Data_BachelorHES\\DataSet_CNC\\DataSetsFolds) \n",
    "\n",
    "class DataDS(Dataset):\n",
    "    \n",
    "    def __init__(self, df, datapath):\n",
    "        self.df=df\n",
    "        self.datapath=str(datapath)\n",
    "        self.duration=5001.24\n",
    "        self.fs=50000\n",
    "        self.channel=4\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row=self.df.iloc[idx]\n",
    "        dirFile=os.path.join(self.datapath,row['FilePath'])\n",
    "        observationLoad=np.load(dirFile)\n",
    "        labelId=row['LabelIDs']\n",
    "        _, lenObs, obs=DataUtils.padTruncate(observationLoad, self.duration, self.fs)\n",
    "        spec, _ = DataUtils.spectro_gram(obs, n_fft=1024, hop_len=None, fs=self.fs)\n",
    "        augspec=DataUtils.spectro_Augment(spec, maxMaskPercentage=0.1,nFreqMask=2, nTimeMasks=2)\n",
    "        return (augspec, labelId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e81b8c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T09:46:44.798052Z",
     "iopub.status.busy": "2025-02-26T09:46:44.797698Z",
     "iopub.status.idle": "2025-02-26T09:46:44.810928Z",
     "shell.execute_reply": "2025-02-26T09:46:44.809859Z"
    },
    "papermill": {
     "duration": 0.026534,
     "end_time": "2025-02-26T09:46:44.812701",
     "exception": false,
     "start_time": "2025-02-26T09:46:44.786167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "#       Spectrogram   |  Shape  (nbChannel, nbFreq, nbTime)|  DataType \n",
    "#Augmented Spec shape |        torch.Size([4, 513, 977])   |  torch.FloatTensor\n",
    "\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class ToolClassifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "        #This convolution block \n",
    "        inChannels=4\n",
    "        outChannels=16\n",
    "        self.conv1 = nn.Conv2d(inChannels, outChannels, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(outChannels)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        inChannels=16\n",
    "        outChannels=32\n",
    "        self.conv2 = nn.Conv2d(inChannels, outChannels, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(outChannels)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Third Convolution Block\n",
    "        inChannels=32\n",
    "        outChannels=64\n",
    "        self.conv3 = nn.Conv2d(inChannels, outChannels, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(outChannels)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Fourth Convolution Block\n",
    "        inChannels=64\n",
    "        outChannels=128\n",
    "        self.conv4 = nn.Conv2d(inChannels, outChannels, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(outChannels)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=outChannels, out_features=11)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c5d377a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T09:46:44.833935Z",
     "iopub.status.busy": "2025-02-26T09:46:44.833685Z",
     "iopub.status.idle": "2025-02-26T09:46:44.847413Z",
     "shell.execute_reply": "2025-02-26T09:46:44.846133Z"
    },
    "papermill": {
     "duration": 0.02651,
     "end_time": "2025-02-26T09:46:44.849148",
     "exception": false,
     "start_time": "2025-02-26T09:46:44.822638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def normalize_ds(dataloader, device):\n",
    "    #Compute mean and standard deviation accross all train data\n",
    "    # results are mean and std for every spectrogram accross all channels (here mean and std have size (4,513,977))\n",
    "    sumInp=None\n",
    "    sumSqDiff=None\n",
    "    totObs=0\n",
    "    \n",
    "    #iterate over data to calculate global data mean\n",
    "    for i, data in enumerate(dataloader): \n",
    "        \n",
    "        inputs =data[0].to(device)\n",
    "        if sumInp is None:\n",
    "            sumInp=torch.zeros(inputs.size(1),inputs.size(2),inputs.size(3), device=device)\n",
    "            sumSqDiff=torch.zeros(inputs.size(1),inputs.size(2),inputs.size(3), device=device)\n",
    "            \n",
    "        sumInp+=inputs.sum(dim=0)\n",
    "        totObs+=inputs.size(0)\n",
    "        del inputs  # ✅ Libère la mémoire\n",
    "        xm.mark_step()  # ✅ Assure la libération sur TPU\n",
    "        \n",
    "    totMean=sumInp/totObs\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs =data[0].to(device)\n",
    "        sumSqDiff+=((inputs-totMean)**2).sum(dim=0)\n",
    "\n",
    "        del inputs  # ✅ Libère la mémoire\n",
    "        xm.mark_step()  # ✅ Assure la libération sur TPU\n",
    "        \n",
    "    totVar=sumSqDiff/totObs\n",
    "    totStd=torch.sqrt(totVar+1e-8)\n",
    "    return totMean, totStd\n",
    "\n",
    "\n",
    "def training(model, device, train_dl, num_epochs):\n",
    "    #store_results\n",
    "    accuracies=[]\n",
    "    losses=[]\n",
    "    #Put the model in training mode \n",
    "    model.train()\n",
    "    # Loss Function, Optimizer and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "    trainMean, trainStd=normalize_ds(train_dl, device)\n",
    "    \n",
    "    # Repeat for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "        \n",
    "        # Repeat for each batch in the training set\n",
    "        for i, data in enumerate(train_dl):\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "            # Normalize the inputs\n",
    "            inputs = (inputs - trainMean.view(1,*trainMean.shape)) / trainStd.view(1,*trainStd.shape)\n",
    "        \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            xm.optimizer_step(optimizer, barrier=True)\n",
    "            scheduler.step()\n",
    "        \n",
    "            # Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "        \n",
    "            #if i % 10 == 0:    # print every 10 mini-batches\n",
    "            #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "            \n",
    "        # Print stats at the end of the epoch\n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction/total_prediction\n",
    "        accuracies.append(acc)\n",
    "        losses.append(avg_loss)\n",
    "        xm.master_print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "    \n",
    "    xm.master_print('Finished Training')\n",
    "    return accuracies, losses, trainMean, trainStd         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17671bd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T09:46:44.871831Z",
     "iopub.status.busy": "2025-02-26T09:46:44.871544Z",
     "iopub.status.idle": "2025-02-26T09:46:47.809084Z",
     "shell.execute_reply": "2025-02-26T09:46:47.807671Z"
    },
    "papermill": {
     "duration": 2.951771,
     "end_time": "2025-02-26T09:46:47.811335",
     "exception": false,
     "start_time": "2025-02-26T09:46:44.859564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "def inference (model, test_dl, device, trainMean, trainStd ):\n",
    "    model.eval() #Put the model in eval/testing mode\n",
    "    allPreds=[]\n",
    "    allLabels=[]\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    \n",
    "    # Disable gradient updates\n",
    "    with torch.no_grad():\n",
    "        for data in test_dl:\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            # Normalize the inputs\n",
    "            inputs = (inputs -  trainMean.view(1,*trainMean.shape)) / trainStd.view(1,*trainStd.shape)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "            allPreds.append(prediction.cpu().numpy())\n",
    "            allLabels.append(labels.cpu().numpy())\n",
    "\n",
    "    #Concatenate all results over the entire test set\n",
    "    allPreds = np.concatenate(allPreds)\n",
    "    allLabels = np.concatenate(allLabels)\n",
    "    \n",
    "    acc = correct_prediction/total_prediction\n",
    "    xm.master_print(f'On inference : Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "\n",
    "    # Compute Confusion Matrix\n",
    "    confMatrix = confusion_matrix(allLabels, allPreds)\n",
    "\n",
    "\n",
    "     # Comput Classification Report (precision, recall, f1-score, support)\n",
    "    classReport = classification_report(allLabels, allPreds,output_dict=True, digits=4, zero_division=0)\n",
    "\n",
    "\n",
    "    return confMatrix, classReport\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd76e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T09:46:47.831817Z",
     "iopub.status.busy": "2025-02-26T09:46:47.831336Z",
     "iopub.status.idle": "2025-02-26T12:10:41.619413Z",
     "shell.execute_reply": "2025-02-26T12:10:41.617666Z"
    },
    "papermill": {
     "duration": 8633.800224,
     "end_time": "2025-02-26T12:10:41.620844",
     "exception": false,
     "start_time": "2025-02-26T09:46:47.820620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1740563209.696227      74 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n",
      "=== Source Location Trace: === \n",
      "learning/45eac/tfrc/runtime/common_lib.cc:239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.08, Accuracy: 0.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.61, Accuracy: 0.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 1.33, Accuracy: 0.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 1.05, Accuracy: 0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.88, Accuracy: 0.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.73, Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 0.61, Accuracy: 0.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 0.50, Accuracy: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 0.39, Accuracy: 0.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 0.30, Accuracy: 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.24, Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.20, Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss: 0.17, Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 0.16, Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 0.14, Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.11, Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 0.10, Accuracy: 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Loss: 0.11, Accuracy: 0.98\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Fold 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On inference : Accuracy: 0.96, Total items: 367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.12, Accuracy: 0.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.62, Accuracy: 0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 1.31, Accuracy: 0.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 1.02, Accuracy: 0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.84, Accuracy: 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.71, Accuracy: 0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 0.57, Accuracy: 0.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 0.46, Accuracy: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 0.37, Accuracy: 0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 0.31, Accuracy: 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.26, Accuracy: 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.21, Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss: 0.19, Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 0.15, Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 0.16, Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.14, Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 0.13, Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Loss: 0.12, Accuracy: 0.97\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Fold 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On inference : Accuracy: 0.98, Total items: 367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.09, Accuracy: 0.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.60, Accuracy: 0.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 1.29, Accuracy: 0.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 1.05, Accuracy: 0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.84, Accuracy: 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.71, Accuracy: 0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 0.57, Accuracy: 0.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 0.48, Accuracy: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 0.40, Accuracy: 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 0.31, Accuracy: 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.25, Accuracy: 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.22, Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss: 0.18, Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 0.17, Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 0.16, Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.13, Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 0.13, Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Loss: 0.13, Accuracy: 0.97\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Fold 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On inference : Accuracy: 0.99, Total items: 367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.06, Accuracy: 0.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.64, Accuracy: 0.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 1.32, Accuracy: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 1.07, Accuracy: 0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.85, Accuracy: 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.73, Accuracy: 0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 0.60, Accuracy: 0.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 0.45, Accuracy: 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 0.35, Accuracy: 0.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 0.28, Accuracy: 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.22, Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.21, Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss: 0.18, Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 0.14, Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 0.13, Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.13, Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 0.11, Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Loss: 0.10, Accuracy: 0.98\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Fold 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On inference : Accuracy: 0.97, Total items: 367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.08, Accuracy: 0.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.57, Accuracy: 0.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 1.26, Accuracy: 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.99, Accuracy: 0.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.79, Accuracy: 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.64, Accuracy: 0.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 0.47, Accuracy: 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 0.36, Accuracy: 0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 0.26, Accuracy: 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 0.21, Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.17, Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.13, Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss: 0.12, Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 0.10, Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 0.09, Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.09, Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 0.08, Accuracy: 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Loss: 0.08, Accuracy: 0.99\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Fold 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On inference : Accuracy: 0.98, Total items: 367\n"
     ]
    }
   ],
   "source": [
    "mainDir='/kaggle/input/mynumpyfiles/NumpyDS'\n",
    "nbFold=5\n",
    "conf_matrix=[]\n",
    "report_dict={}\n",
    "for i in range(nbFold):\n",
    "    #myTrain contains the data to be fed to the DataDS class for batch processing later on\n",
    "    myTrain=myGlobalDs[(myGlobalDs['Fold']==i) & (myGlobalDs['Type']=='Train')].copy()\n",
    "    myTrain=myTrain.drop(columns=['Fold', 'Type', 'nb', 'condLabel'])\n",
    "    \n",
    "    # myDs is an instance of DataDS, constructor needs :\n",
    "    #         A dataframe with at least relative paths in a column 'FilePath' and a column 'LabelIDs'\n",
    "    #         An absolute path which is a level above the relative path mentioned in the dataframe \n",
    "    trainFoldi=DataDS(myTrain, mainDir)\n",
    "    train_dataloader = DataLoader(trainFoldi, batch_size=8, shuffle=True)\n",
    "    \n",
    "    #Do the same loading for the test set\n",
    "    myTest=myGlobalDs[(myGlobalDs['Fold']==i) & (myGlobalDs['Type']=='Test')].copy()\n",
    "    testFoldi=DataDS(myTest, mainDir)\n",
    "    test_dataloader = DataLoader(testFoldi, batch_size=8, shuffle=False)\n",
    "    \n",
    "    # Create the model and put it on the GPU if available\n",
    "    myModel = ToolClassifier()\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #With TPU\n",
    "    device = xm.xla_device()\n",
    "    train_dataloader=MpDeviceLoader(train_dataloader, device)\n",
    "    test_dataloader=MpDeviceLoader(test_dataloader, device)\n",
    "    myModel = myModel.to(device)\n",
    "    # Check that it is on Cuda\n",
    "    next(myModel.parameters()).device\n",
    "  \n",
    "    num_epochs=18   # Exctracted by learning curve on Epochs\n",
    "    accuracies, losses, trainMean, trainStd=training(myModel, device, train_dataloader, num_epochs)\n",
    "    \n",
    "    # Run inference on trained model with the test set\n",
    "    print(f\"Results for Fold {i}\")\n",
    "    confMatrix, report=inference(myModel, test_dataloader,device, trainMean, trainStd)\n",
    "    conf_matrix.append(confMatrix)\n",
    "    report_df=pd.DataFrame(report).transpose()\n",
    "    report_dict[f\"Fold {i}\"]=report_df\n",
    "\n",
    "#PyTorch class weight APRES \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b107fe34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:41.650735Z",
     "iopub.status.busy": "2025-02-26T12:10:41.650478Z",
     "iopub.status.idle": "2025-02-26T12:10:41.655280Z",
     "shell.execute_reply": "2025-02-26T12:10:41.654123Z"
    },
    "papermill": {
     "duration": 0.021711,
     "end_time": "2025-02-26T12:10:41.656879",
     "exception": false,
     "start_time": "2025-02-26T12:10:41.635168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataDS\t DataLoader\t DataUtils\t Dataset\t F\t MpDeviceLoader\t ToolClassifier\t accuracies\t classification_report\t \n",
      "confMatrix\t conf_matrix\t confusion_matrix\t csv\t device\t dirname\t filenames\t i\t inference\t \n",
      "init\t losses\t mainDir\t math\t met\t myGlobalDs\t myMeta\t myModel\t myTest\t \n",
      "myTrain\t nbFold\t nn\t normalize_ds\t np\t num_epochs\t os\t pd\t pl\t \n",
      "plt\t random\t report\t report_df\t report_dict\t testFoldi\t test_dataloader\t test_utils\t torch\t \n",
      "torch_xla\t torchaudio\t trainFoldi\t trainMean\t trainStd\t train_dataloader\t training\t transforms\t warnings\t \n",
      "xm\t xmp\t xr\t xu\t \n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53200462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:41.685886Z",
     "iopub.status.busy": "2025-02-26T12:10:41.685625Z",
     "iopub.status.idle": "2025-02-26T12:10:41.688789Z",
     "shell.execute_reply": "2025-02-26T12:10:41.688064Z"
    },
    "papermill": {
     "duration": 0.019971,
     "end_time": "2025-02-26T12:10:41.690546",
     "exception": false,
     "start_time": "2025-02-26T12:10:41.670575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#report_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef586de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:41.719791Z",
     "iopub.status.busy": "2025-02-26T12:10:41.719574Z",
     "iopub.status.idle": "2025-02-26T12:10:41.731061Z",
     "shell.execute_reply": "2025-02-26T12:10:41.730326Z"
    },
    "papermill": {
     "duration": 0.028499,
     "end_time": "2025-02-26T12:10:41.732755",
     "exception": false,
     "start_time": "2025-02-26T12:10:41.704256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i, matrix in enumerate(conf_matrix):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(f\"/kaggle/working/confusion_matrix_CNN_fold_{i+1}.csv\", index=False)\n",
    "del i, matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a97b540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:41.762060Z",
     "iopub.status.busy": "2025-02-26T12:10:41.761835Z",
     "iopub.status.idle": "2025-02-26T12:10:41.768745Z",
     "shell.execute_reply": "2025-02-26T12:10:41.767829Z"
    },
    "papermill": {
     "duration": 0.023554,
     "end_time": "2025-02-26T12:10:41.770424",
     "exception": false,
     "start_time": "2025-02-26T12:10:41.746870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i, report in enumerate(report_dict):\n",
    "    report_dict[report].to_csv(f\"/kaggle/working/Reports_CNN_Fold_{i+1}.csv\", index=True)\n",
    "del i, report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25f96ae",
   "metadata": {
    "papermill": {
     "duration": 0.014112,
     "end_time": "2025-02-26T12:10:41.798730",
     "exception": false,
     "start_time": "2025-02-26T12:10:41.784618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Learning curve with respect to number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f9379cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:41.827976Z",
     "iopub.status.busy": "2025-02-26T12:10:41.827768Z",
     "iopub.status.idle": "2025-02-26T12:10:41.835084Z",
     "shell.execute_reply": "2025-02-26T12:10:41.834294Z"
    },
    "papermill": {
     "duration": 0.024472,
     "end_time": "2025-02-26T12:10:41.836934",
     "exception": false,
     "start_time": "2025-02-26T12:10:41.812462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import torch\\nimport torch_xla\\nimport torch.nn as nn\\nimport torch_xla.core.xla_model as xm\\nimport torch_xla.distributed.parallel_loader as pl\\nimport torch_xla.utils.utils as xu\\nfrom torch.utils.data import DataLoader\\n\\n#WITH TPU\\nfrom torch_xla.distributed.parallel_loader import MpDeviceLoader\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n# ----------------------------\\n# Training Loop\\n# ----------------------------\\n\\n\\ndef training_with_learning_curve(model,device, train_dl, test_dl, num_epochs=12):\\n\\n    # Lists to store results\\n    train_losses = []\\n    test_losses = []\\n    train_accuracies = []\\n    test_accuracies = []\\n\\n    model = model.to(device)\\n    model.train()\\n\\n    # Loss Function, Optimizer and Scheduler\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\\n                                                steps_per_epoch=int(len(train_dl)),\\n                                                epochs=num_epochs,\\n                                                anneal_strategy='linear')\\n    trainMean, trainStd=normalize_ds(train_dl, device) #Find global mean / std for training set\\n\\n    # Repeat for each epoch\\n    for epoch in range(num_epochs):\\n        #Put the model in training mode \\n        model.train()\\n        running_loss = 0.0\\n        correct_prediction = 0\\n        total_prediction = 0\\n        \\n        # Repeat for each batch in the training set\\n        for i, data in enumerate(train_dl):\\n            # Get the input features and target labels, and put them on the GPU\\n            inputs, labels = data[0].to(device), data[1].to(device)\\n        \\n            # Normalize the inputs\\n            inputs = (inputs - trainMean.view(1,*trainMean.shape)) / trainStd.view(1,*trainStd.shape)\\n        \\n            # Zero the parameter gradients\\n            optimizer.zero_grad()\\n        \\n            # forward + backward + optimize\\n            outputs = model(inputs)\\n            loss = criterion(outputs, labels)\\n            loss.backward()\\n            \\n\\n            #Use ofd TPU + Scheduler\\n            xm.optimizer_step(optimizer, barrier=True)\\n            xm.mark_step()  # Synchronisation du TPU\\n            scheduler.step()\\n        \\n            # Keep stats for Loss and Accuracy\\n            running_loss += loss.item()\\n        \\n            # Get the predicted class with the highest score\\n            _, prediction = torch.max(outputs,1)\\n            # Count of predictions that matched the target label\\n            correct_prediction += (prediction == labels).sum().item()\\n            total_prediction += prediction.shape[0]\\n                \\n        # Print stats at the end of the epoch for the training stage\\n        num_batches = len(train_dl)\\n        avg_loss = running_loss / num_batches\\n        acc = correct_prediction/total_prediction\\n        xm.master_print(f'Epoch Training: {epoch}, Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}')\\n        train_losses.append(avg_loss)\\n        train_accuracies.append(acc)\\n\\n        #Evaluate model for each epoch\\n        model.eval()\\n        test_loss = 0.0\\n        correct=0\\n        total=0\\n        \\n        with torch.no_grad():     \\n            for inputs, labels in test_dl:\\n                inputs, labels = inputs.to(device), labels.to(device)\\n                \\n                # Normalize the inputs\\n                inputs = (inputs - trainMean.view(1,*trainMean.shape)) / trainStd.view(1,*trainStd.shape)\\n\\n                #Get prediciitons\\n                outputs = model(inputs)\\n                _, prediction = torch.max(outputs, 1)\\n                \\n                loss = criterion(outputs, labels)\\n                test_loss += loss.item()\\n\\n                correct += (prediction == labels).sum().item()\\n                total += prediction.shape[0]\\n\\n                               \\n        test_acc = correct/total\\n        avg_test_loss=test_loss / len(test_dl)\\n        test_losses.append(avg_test_loss)\\n        test_accuracies.append(test_acc)\\n        xm.master_print(f'Epoch Test: {epoch}, Loss: {avg_test_loss:.4f}, Accuracy: {test_acc:.4f}')\\n    \\n    xm.master_print('Finished Training')\\n    return train_losses, train_accuracies, test_losses, test_accuracies\\n    \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import torch\n",
    "import torch_xla\n",
    "import torch.nn as nn\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.utils.utils as xu\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#WITH TPU\n",
    "from torch_xla.distributed.parallel_loader import MpDeviceLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "def training_with_learning_curve(model,device, train_dl, test_dl, num_epochs=12):\n",
    "\n",
    "    # Lists to store results\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Loss Function, Optimizer and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "    trainMean, trainStd=normalize_ds(train_dl, device) #Find global mean / std for training set\n",
    "\n",
    "    # Repeat for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        #Put the model in training mode \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "        \n",
    "        # Repeat for each batch in the training set\n",
    "        for i, data in enumerate(train_dl):\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "            # Normalize the inputs\n",
    "            inputs = (inputs - trainMean.view(1,*trainMean.shape)) / trainStd.view(1,*trainStd.shape)\n",
    "        \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "\n",
    "            #Use ofd TPU + Scheduler\n",
    "            xm.optimizer_step(optimizer, barrier=True)\n",
    "            xm.mark_step()  # Synchronisation du TPU\n",
    "            scheduler.step()\n",
    "        \n",
    "            # Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "                \n",
    "        # Print stats at the end of the epoch for the training stage\n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction/total_prediction\n",
    "        xm.master_print(f'Epoch Training: {epoch}, Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}')\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(acc)\n",
    "\n",
    "        #Evaluate model for each epoch\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct=0\n",
    "        total=0\n",
    "        \n",
    "        with torch.no_grad():     \n",
    "            for inputs, labels in test_dl:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Normalize the inputs\n",
    "                inputs = (inputs - trainMean.view(1,*trainMean.shape)) / trainStd.view(1,*trainStd.shape)\n",
    "\n",
    "                #Get prediciitons\n",
    "                outputs = model(inputs)\n",
    "                _, prediction = torch.max(outputs, 1)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                correct += (prediction == labels).sum().item()\n",
    "                total += prediction.shape[0]\n",
    "\n",
    "                               \n",
    "        test_acc = correct/total\n",
    "        avg_test_loss=test_loss / len(test_dl)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "        xm.master_print(f'Epoch Test: {epoch}, Loss: {avg_test_loss:.4f}, Accuracy: {test_acc:.4f}')\n",
    "    \n",
    "    xm.master_print('Finished Training')\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc7f29f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:41.866401Z",
     "iopub.status.busy": "2025-02-26T12:10:41.866159Z",
     "iopub.status.idle": "2025-02-26T12:10:41.873487Z",
     "shell.execute_reply": "2025-02-26T12:10:41.872587Z"
    },
    "papermill": {
     "duration": 0.024539,
     "end_time": "2025-02-26T12:10:41.875413",
     "exception": false,
     "start_time": "2025-02-26T12:10:41.850874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n#Show learning curve with respect to epochs\\n\\nmainDir=\\'/kaggle/input/mynumpyfiles/NumpyDS\\'\\n\\n\\n\\nimport random\\n\\ntorch.manual_seed(42)\\ntorch_xla.utils.utils.set_rng_state(42)\\nnp.random.seed(42)\\nrandom.seed(42)\\n\\nnbFold=5\\nnum_epochs=50\\n#shapes are (num_runs, epochs)\\nall_train_losses = []\\nall_test_losses = []\\nall_train_accuracies = []\\nall_test_accuracies = []\\n\\nfor run in range(nbFold):\\n    xm.master_print(f\"\\n Run {run+1}/{nbFold}...\\n\")\\n    myTrain=myGlobalDs[(myGlobalDs[\\'Fold\\']==run) & (myGlobalDs[\\'Type\\']==\\'Train\\')].copy()\\n    myTrain=myTrain.drop(columns=[\\'Fold\\', \\'Type\\', \\'nb\\', \\'condLabel\\'])\\n    \\n    # DataDs is an instance of DataDS, constructor needs :\\n    #         A dataframe with at least relative paths in a column \\'FilePath\\' and a column \\'LabelIDs\\'\\n    #         An absolute path which is a level above the relative path mentioned in the dataframe \\n    trainFoldi=DataDS(myTrain, mainDir)\\n    train_dataloader = DataLoader(trainFoldi, batch_size=16, shuffle=True)\\n\\n    #Do the same loading for the test set\\n    myTest=myGlobalDs[(myGlobalDs[\\'Fold\\']==run) & (myGlobalDs[\\'Type\\']==\\'Test\\')].copy()\\n    myTest=myTest.drop(columns=[\\'Fold\\', \\'Type\\', \\'nb\\', \\'condLabel\\'])\\n    testFoldi=DataDS(myTest, mainDir)\\n    test_dataloader = DataLoader(testFoldi, batch_size=8, shuffle=True)\\n    \\n   \\n    #WITH TPU\\n    device=xm.xla_device()\\n    train_dataloader=MpDeviceLoader(train_dataloader, device)\\n    test_dataloader=MpDeviceLoader(test_dataloader, device)\\n     # Create the model and put it on the GPU if available\\n    myModel = ToolClassifier()\\n    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\\n    myModel = myModel.to(device)\\n    # Check that it is on Cuda\\n    xm.master_print(f\"Model is on: {next(myModel.parameters()).device}\")\\n\\n\\n    train_losses, train_accuracies, test_losses, test_accuracies = training_with_learning_curve(myModel,device, train_dataloader, test_dataloader, num_epochs)\\n    all_train_losses.append(train_losses)\\n    all_test_losses.append(test_losses)\\n    all_train_accuracies.append(train_accuracies)\\n    all_test_accuracies.append(test_accuracies)\\n\\n    # ✅ Libérer la mémoire TPU après chaque fold\\n    del myModel\\n    gc.collect()\\n    xm.rendezvous(\"cleanup\") \\n\\nall_train_losses = np.array(all_train_losses)\\nall_test_losses = np.array(all_test_losses)\\nall_train_accuracies = np.array(all_train_accuracies)\\nall_test_accuracies = np.array(all_test_accuracies)\\n\\nnp.save(\"/kaggle/working/epoch_train_losses.npy\", all_train_losses)\\nnp.save(\"/kaggle/working/epoch_test_losses.npy\", all_test_losses)\\nnp.save(\"/kaggle/working/epoch_train_accuracies.npy\", all_train_accuracies)\\nnp.save(\"/kaggle/working/epoch_test_accuracies.npy\", all_test_accuracies)\\n\\ntrain_loss_mean, train_loss_std=np.mean(all_train_losses, axis=0), np.std(all_train_losses, axis=0)\\ntest_loss_mean, test_loss_std=np.mean(all_test_losses, axis=0), np.std(all_test_losses, axis=0)\\ntrain_accuracy_mean, train_accuracy_std=np.mean(all_train_accuracies, axis=0), np.std(all_train_accuracies, axis=0)\\ntest_accuracy_mean, test_accuracy_std=np.mean(all_train_accuracies, axis=0), np.std(all_train_accuracies, axis=0)\\n\\n\\nepochs=range(1, num_epochs+1)\\nplt.figure(figsize=(12, 5))\\n\\n# Courbe de la Loss\\nplt.subplot(1, 2, 1)\\nplt.plot(epochs, train_loss_mean, label=\"Train Loss\", color=\"blue\")\\nplt.fill_between(epochs, train_loss_mean-train_loss_std,train_loss_mean+train_loss_std,alpha=0.2, color=\"blue\")\\n\\n\\nplt.plot(epochs, test_loss_mean, label=\"Test Loss\", color=\"orange\")\\nplt.fill_between(epochs, test_loss_mean-test_loss_std,test_loss_mean+test_loss_std,alpha=0.2, color=\"orange\")\\n\\nplt.xlabel(\"Epochs\")\\nplt.ylabel(\"Loss\")\\nplt.title(\"Learning Curve - Loss with Standard Deviation\")\\nplt.legend()\\n\\n# Courbe de l\\'Accuracy\\nplt.subplot(1, 2, 2)\\nplt.plot(epochs, train_accuracy_mean, label=\"Train Accuracy\", color=\"blue\")\\nplt.fill_between(epochs, train_accuracy_mean-train_accuracy_std,train_accuracy_mean+train_accuracy_std,alpha=0.2, color=\"blue\")\\n\\n\\nplt.plot(epochs, test_accuracy_mean, label=\"Test Accuracy\", color=\"orange\")\\nplt.fill_between(epochs, test_accuracy_mean-test_accuracy_std,test_accuracy_mean+test_accuracy_std,alpha=0.2, color=\"orange\")\\n\\nplt.xlabel(\"Epochs\")\\nplt.ylabel(\"Accuracy\")\\nplt.title(\"Learning Curve - Accuracy with standard deciation\")\\nplt.legend()\\n\\nplt.show()\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "#Show learning curve with respect to epochs\n",
    "\n",
    "mainDir='/kaggle/input/mynumpyfiles/NumpyDS'\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch_xla.utils.utils.set_rng_state(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "nbFold=5\n",
    "num_epochs=50\n",
    "#shapes are (num_runs, epochs)\n",
    "all_train_losses = []\n",
    "all_test_losses = []\n",
    "all_train_accuracies = []\n",
    "all_test_accuracies = []\n",
    "\n",
    "for run in range(nbFold):\n",
    "    xm.master_print(f\"\\n Run {run+1}/{nbFold}...\\n\")\n",
    "    myTrain=myGlobalDs[(myGlobalDs['Fold']==run) & (myGlobalDs['Type']=='Train')].copy()\n",
    "    myTrain=myTrain.drop(columns=['Fold', 'Type', 'nb', 'condLabel'])\n",
    "    \n",
    "    # DataDs is an instance of DataDS, constructor needs :\n",
    "    #         A dataframe with at least relative paths in a column 'FilePath' and a column 'LabelIDs'\n",
    "    #         An absolute path which is a level above the relative path mentioned in the dataframe \n",
    "    trainFoldi=DataDS(myTrain, mainDir)\n",
    "    train_dataloader = DataLoader(trainFoldi, batch_size=16, shuffle=True)\n",
    "\n",
    "    #Do the same loading for the test set\n",
    "    myTest=myGlobalDs[(myGlobalDs['Fold']==run) & (myGlobalDs['Type']=='Test')].copy()\n",
    "    myTest=myTest.drop(columns=['Fold', 'Type', 'nb', 'condLabel'])\n",
    "    testFoldi=DataDS(myTest, mainDir)\n",
    "    test_dataloader = DataLoader(testFoldi, batch_size=8, shuffle=True)\n",
    "    \n",
    "   \n",
    "    #WITH TPU\n",
    "    device=xm.xla_device()\n",
    "    train_dataloader=MpDeviceLoader(train_dataloader, device)\n",
    "    test_dataloader=MpDeviceLoader(test_dataloader, device)\n",
    "     # Create the model and put it on the GPU if available\n",
    "    myModel = ToolClassifier()\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    myModel = myModel.to(device)\n",
    "    # Check that it is on Cuda\n",
    "    xm.master_print(f\"Model is on: {next(myModel.parameters()).device}\")\n",
    "\n",
    "\n",
    "    train_losses, train_accuracies, test_losses, test_accuracies = training_with_learning_curve(myModel,device, train_dataloader, test_dataloader, num_epochs)\n",
    "    all_train_losses.append(train_losses)\n",
    "    all_test_losses.append(test_losses)\n",
    "    all_train_accuracies.append(train_accuracies)\n",
    "    all_test_accuracies.append(test_accuracies)\n",
    "\n",
    "    # ✅ Libérer la mémoire TPU après chaque fold\n",
    "    del myModel\n",
    "    gc.collect()\n",
    "    xm.rendezvous(\"cleanup\") \n",
    "\n",
    "all_train_losses = np.array(all_train_losses)\n",
    "all_test_losses = np.array(all_test_losses)\n",
    "all_train_accuracies = np.array(all_train_accuracies)\n",
    "all_test_accuracies = np.array(all_test_accuracies)\n",
    "\n",
    "np.save(\"/kaggle/working/epoch_train_losses.npy\", all_train_losses)\n",
    "np.save(\"/kaggle/working/epoch_test_losses.npy\", all_test_losses)\n",
    "np.save(\"/kaggle/working/epoch_train_accuracies.npy\", all_train_accuracies)\n",
    "np.save(\"/kaggle/working/epoch_test_accuracies.npy\", all_test_accuracies)\n",
    "\n",
    "train_loss_mean, train_loss_std=np.mean(all_train_losses, axis=0), np.std(all_train_losses, axis=0)\n",
    "test_loss_mean, test_loss_std=np.mean(all_test_losses, axis=0), np.std(all_test_losses, axis=0)\n",
    "train_accuracy_mean, train_accuracy_std=np.mean(all_train_accuracies, axis=0), np.std(all_train_accuracies, axis=0)\n",
    "test_accuracy_mean, test_accuracy_std=np.mean(all_train_accuracies, axis=0), np.std(all_train_accuracies, axis=0)\n",
    "\n",
    "\n",
    "epochs=range(1, num_epochs+1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Courbe de la Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss_mean, label=\"Train Loss\", color=\"blue\")\n",
    "plt.fill_between(epochs, train_loss_mean-train_loss_std,train_loss_mean+train_loss_std,alpha=0.2, color=\"blue\")\n",
    "\n",
    "\n",
    "plt.plot(epochs, test_loss_mean, label=\"Test Loss\", color=\"orange\")\n",
    "plt.fill_between(epochs, test_loss_mean-test_loss_std,test_loss_mean+test_loss_std,alpha=0.2, color=\"orange\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve - Loss with Standard Deviation\")\n",
    "plt.legend()\n",
    "\n",
    "# Courbe de l'Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracy_mean, label=\"Train Accuracy\", color=\"blue\")\n",
    "plt.fill_between(epochs, train_accuracy_mean-train_accuracy_std,train_accuracy_mean+train_accuracy_std,alpha=0.2, color=\"blue\")\n",
    "\n",
    "\n",
    "plt.plot(epochs, test_accuracy_mean, label=\"Test Accuracy\", color=\"orange\")\n",
    "plt.fill_between(epochs, test_accuracy_mean-test_accuracy_std,test_accuracy_mean+test_accuracy_std,alpha=0.2, color=\"orange\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve - Accuracy with standard deciation\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8220c5e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:41.937449Z",
     "iopub.status.busy": "2025-02-26T12:10:41.937206Z",
     "iopub.status.idle": "2025-02-26T12:10:41.941976Z",
     "shell.execute_reply": "2025-02-26T12:10:41.940923Z"
    },
    "papermill": {
     "duration": 0.0217,
     "end_time": "2025-02-26T12:10:41.943144",
     "exception": false,
     "start_time": "2025-02-26T12:10:41.921444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport gc\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torch_xla.core.xla_model as xm\\nimport torch_xla.distributed.parallel_loader as pl\\nimport torch_xla.distributed.xla_multiprocessing as xmp\\nfrom torch.utils.data import DataLoader, DistributedSampler'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "from torch.utils.data import DataLoader, DistributedSampler\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c187f8dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:41.973561Z",
     "iopub.status.busy": "2025-02-26T12:10:41.973349Z",
     "iopub.status.idle": "2025-02-26T12:10:41.979727Z",
     "shell.execute_reply": "2025-02-26T12:10:41.978980Z"
    },
    "papermill": {
     "duration": 0.023746,
     "end_time": "2025-02-26T12:10:41.981521",
     "exception": false,
     "start_time": "2025-02-26T12:10:41.957775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ndef training_one_epoch(model, optimizer, criterion, scheduler, device, train_dl, trainMean, trainStd, epochNb):\\n        #Put the model in training mode \\n    model.train()\\n    running_loss = 0.0\\n    correct_prediction = 0\\n    total_prediction = 0\\n    \\n    # Repeat for each batch in the training set\\n    for i, data in enumerate(train_dl):\\n        # Get the input features and target labels, and put them on the GPU\\n        inputs, labels = data[0].to(device), data[1].to(device)\\n    \\n        # Normalize the inputs\\n        inputs = (inputs - trainMean.view(1,*trainMean.shape)) / trainStd.view(1,*trainStd.shape)\\n    \\n        # Zero the parameter gradients\\n        optimizer.zero_grad()\\n    \\n        # forward + backward + optimize\\n        outputs = model(inputs)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        \\n\\n        #Use ofd TPU + Scheduler\\n        xm.optimizer_step(optimizer, barrier=True)\\n        xm.mark_step()  # Synchronisation du TPU\\n        scheduler.step()\\n    \\n        # Keep stats for Loss and Accuracy\\n        running_loss += loss.item()\\n    \\n        # Get the predicted class with the highest score\\n        _, prediction = torch.max(outputs,1)\\n        # Count of predictions that matched the target label\\n        correct_prediction += (prediction == labels).sum().item()\\n        total_prediction += prediction.shape[0]\\n            \\n    # Print stats at the end of the epoch for the training stage\\n    num_batches = len(train_dl)\\n    avg_loss = running_loss / num_batches\\n    acc = correct_prediction/total_prediction\\n    xm.master_print(f'Epoch Training: {epochNb}, Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}')\\n    return avg_loss, acc\\n\\ndef testing_one_epoch(model, criterion, device, test_dl, trainMean, trainStd, epochNb):\\n    #Evaluate model for each epoch\\n    model.eval()\\n    test_loss = 0.0\\n    correct=0\\n    total=0\\n    \\n    with torch.no_grad():     \\n        for inputs, labels in test_dl:\\n            inputs, labels = inputs.to(device), labels.to(device)\\n            \\n            # Normalize the inputs\\n            inputs = (inputs - trainMean.view(1,*trainMean.shape)) / trainStd.view(1,*trainStd.shape)\\n\\n            #Get prediciitons\\n            outputs = model(inputs)\\n            _, prediction = torch.max(outputs, 1)\\n            \\n            loss = criterion(outputs, labels)\\n            test_loss += loss.item()\\n\\n            correct += (prediction == labels).sum().item()\\n            total += prediction.shape[0]\\n\\n                           \\n    test_acc = correct/total\\n    avg_test_loss=test_loss / len(test_dl)\\n    xm.master_print(f'Epoch Test: {epochNb}, Loss: {avg_test_loss:.4f}, Accuracy: {test_acc:.4f}')\\n    return avg_test_loss, test_acc\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "def training_one_epoch(model, optimizer, criterion, scheduler, device, train_dl, trainMean, trainStd, epochNb):\n",
    "        #Put the model in training mode \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    \n",
    "    # Repeat for each batch in the training set\n",
    "    for i, data in enumerate(train_dl):\n",
    "        # Get the input features and target labels, and put them on the GPU\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    \n",
    "        # Normalize the inputs\n",
    "        inputs = (inputs - trainMean.view(1,*trainMean.shape)) / trainStd.view(1,*trainStd.shape)\n",
    "    \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        #Use ofd TPU + Scheduler\n",
    "        xm.optimizer_step(optimizer, barrier=True)\n",
    "        xm.mark_step()  # Synchronisation du TPU\n",
    "        scheduler.step()\n",
    "    \n",
    "        # Keep stats for Loss and Accuracy\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "            \n",
    "    # Print stats at the end of the epoch for the training stage\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    xm.master_print(f'Epoch Training: {epochNb}, Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}')\n",
    "    return avg_loss, acc\n",
    "\n",
    "def testing_one_epoch(model, criterion, device, test_dl, trainMean, trainStd, epochNb):\n",
    "    #Evaluate model for each epoch\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct=0\n",
    "    total=0\n",
    "    \n",
    "    with torch.no_grad():     \n",
    "        for inputs, labels in test_dl:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Normalize the inputs\n",
    "            inputs = (inputs - trainMean.view(1,*trainMean.shape)) / trainStd.view(1,*trainStd.shape)\n",
    "\n",
    "            #Get prediciitons\n",
    "            outputs = model(inputs)\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            correct += (prediction == labels).sum().item()\n",
    "            total += prediction.shape[0]\n",
    "\n",
    "                           \n",
    "    test_acc = correct/total\n",
    "    avg_test_loss=test_loss / len(test_dl)\n",
    "    xm.master_print(f'Epoch Test: {epochNb}, Loss: {avg_test_loss:.4f}, Accuracy: {test_acc:.4f}')\n",
    "    return avg_test_loss, test_acc\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fdd673a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:42.012245Z",
     "iopub.status.busy": "2025-02-26T12:10:42.012019Z",
     "iopub.status.idle": "2025-02-26T12:10:42.018532Z",
     "shell.execute_reply": "2025-02-26T12:10:42.017734Z"
    },
    "papermill": {
     "duration": 0.024234,
     "end_time": "2025-02-26T12:10:42.020331",
     "exception": false,
     "start_time": "2025-02-26T12:10:41.996097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def process_one_fold(run, device, results_queue, myGlobalDs, mainDir, nbEpochs):\\n    \\n\\n    # 📌 Chargement des données\\n    myTrain = myGlobalDs[(myGlobalDs[\\'Fold\\'] == run) & (myGlobalDs[\\'Type\\'] == \\'Train\\')].copy()\\n    myTrain = myTrain.drop(columns=[\\'Fold\\', \\'Type\\', \\'nb\\', \\'condLabel\\'])\\n    trainFoldi = DataDS(myTrain, mainDir)\\n\\n    myTest = myGlobalDs[(myGlobalDs[\\'Fold\\'] == run) & (myGlobalDs[\\'Type\\'] == \\'Test\\')].copy()\\n    myTest = myTest.drop(columns=[\\'Fold\\', \\'Type\\', \\'nb\\', \\'condLabel\\'])\\n    testFoldi = DataDS(myTest, mainDir)\\n\\n    # Distribute datasets on 8 TPU Cores\\n    train_sampler = DistributedSampler(trainFoldi, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=True)\\n    test_sampler = DistributedSampler(testFoldi, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=False)\\n\\n    # Create the DataLoaders\\n    train_dataloader = DataLoader(trainFoldi, batch_size=8, sampler=train_sampler)\\n    test_dataloader = DataLoader(testFoldi, batch_size=8, sampler=test_sampler)\\n\\n    # Create the model to be trained\\n    myModel = ToolClassifier().to(device)\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = optim.Adam(myModel.parameters(), lr=0.001)\\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_dataloader), epochs=nbEpochs, anneal_strategy=\\'linear\\')\\n\\n    # Get global train mean an dstd for normalization\\n    trainMean, trainStd = normalize_ds(train_dataloader, device)\\n    \\n    # List to store results\\n    train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\\n\\n    # train -> test -> train loop for each epoch\\n    #training_one_epoch(model, optimizer, criterion, scheduler, device, train_dl, trainMean, trainStd, epochNb)\\n    #testing_one_epoch(model, criterion, device, test_dl, trainMean, trainStd, epochNb)\\n    for epoch in range(nbEpochs):\\n        train_loss, train_acc = training_one_epoch(myModel, optimizer, criterion, scheduler, device, train_dataloader, trainMean, trainStd, epoch)\\n        test_loss, test_acc = testing_one_epoch(myModel, criterion, device, test_dataloader, trainMean, trainStd, epoch)\\n\\n        train_losses.append(train_loss)\\n        test_losses.append(test_loss)\\n        train_accuracies.append(train_acc)\\n        test_accuracies.append(test_acc)\\n\\n        torch.cuda.empty_cache()  # Libère la mémoire GPU si applicable\\n        gc.collect()  # Libère la mémoire du CPU\\n        xm.mark_step()  # Libère la mémoire TPU\\n\\n        # A TPU sync at the end of an epoch \\n        xm.rendezvous(\"epoch_sync\")\\n\\n    xm.master_print(\"Finished Training !\")\\n    # Send results to main process\\n    results_queue.put((train_losses, test_losses, train_accuracies, test_accuracies))\\n\\n    # Cleanup of TPU memory\\n    del myModel\\n    gc.collect()\\n    xm.mark_step()  \\n    xm.rendezvous(\"cleanup\")'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def process_one_fold(run, device, results_queue, myGlobalDs, mainDir, nbEpochs):\n",
    "    \n",
    "\n",
    "    # 📌 Chargement des données\n",
    "    myTrain = myGlobalDs[(myGlobalDs['Fold'] == run) & (myGlobalDs['Type'] == 'Train')].copy()\n",
    "    myTrain = myTrain.drop(columns=['Fold', 'Type', 'nb', 'condLabel'])\n",
    "    trainFoldi = DataDS(myTrain, mainDir)\n",
    "\n",
    "    myTest = myGlobalDs[(myGlobalDs['Fold'] == run) & (myGlobalDs['Type'] == 'Test')].copy()\n",
    "    myTest = myTest.drop(columns=['Fold', 'Type', 'nb', 'condLabel'])\n",
    "    testFoldi = DataDS(myTest, mainDir)\n",
    "\n",
    "    # Distribute datasets on 8 TPU Cores\n",
    "    train_sampler = DistributedSampler(trainFoldi, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=True)\n",
    "    test_sampler = DistributedSampler(testFoldi, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=False)\n",
    "\n",
    "    # Create the DataLoaders\n",
    "    train_dataloader = DataLoader(trainFoldi, batch_size=8, sampler=train_sampler)\n",
    "    test_dataloader = DataLoader(testFoldi, batch_size=8, sampler=test_sampler)\n",
    "\n",
    "    # Create the model to be trained\n",
    "    myModel = ToolClassifier().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(myModel.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_dataloader), epochs=nbEpochs, anneal_strategy='linear')\n",
    "\n",
    "    # Get global train mean an dstd for normalization\n",
    "    trainMean, trainStd = normalize_ds(train_dataloader, device)\n",
    "    \n",
    "    # List to store results\n",
    "    train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
    "\n",
    "    # train -> test -> train loop for each epoch\n",
    "    #training_one_epoch(model, optimizer, criterion, scheduler, device, train_dl, trainMean, trainStd, epochNb)\n",
    "    #testing_one_epoch(model, criterion, device, test_dl, trainMean, trainStd, epochNb)\n",
    "    for epoch in range(nbEpochs):\n",
    "        train_loss, train_acc = training_one_epoch(myModel, optimizer, criterion, scheduler, device, train_dataloader, trainMean, trainStd, epoch)\n",
    "        test_loss, test_acc = testing_one_epoch(myModel, criterion, device, test_dataloader, trainMean, trainStd, epoch)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        torch.cuda.empty_cache()  # Libère la mémoire GPU si applicable\n",
    "        gc.collect()  # Libère la mémoire du CPU\n",
    "        xm.mark_step()  # Libère la mémoire TPU\n",
    "\n",
    "        # A TPU sync at the end of an epoch \n",
    "        xm.rendezvous(\"epoch_sync\")\n",
    "\n",
    "    xm.master_print(\"Finished Training !\")\n",
    "    # Send results to main process\n",
    "    results_queue.put((train_losses, test_losses, train_accuracies, test_accuracies))\n",
    "\n",
    "    # Cleanup of TPU memory\n",
    "    del myModel\n",
    "    gc.collect()\n",
    "    xm.mark_step()  \n",
    "    xm.rendezvous(\"cleanup\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f612e57a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:42.051160Z",
     "iopub.status.busy": "2025-02-26T12:10:42.050936Z",
     "iopub.status.idle": "2025-02-26T12:10:42.056093Z",
     "shell.execute_reply": "2025-02-26T12:10:42.054977Z"
    },
    "papermill": {
     "duration": 0.022734,
     "end_time": "2025-02-26T12:10:42.057667",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.034933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def _mp_fn(index, results_queue, nbFold, myGlobalDS, mainDir, num_epochs):\\n    device = xm.xla_device()\\n    for run in range(nbFold):\\n        xm.master_print(f\"\\n Run {run+1}/{nbFold} sur TPU Core {xm.get_ordinal()}...\\n\")\\n        process_one_fold(run, device, results_queue, myGlobalDS, mainDir, num_epochs)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def _mp_fn(index, results_queue, nbFold, myGlobalDS, mainDir, num_epochs):\n",
    "    device = xm.xla_device()\n",
    "    for run in range(nbFold):\n",
    "        xm.master_print(f\"\\n Run {run+1}/{nbFold} sur TPU Core {xm.get_ordinal()}...\\n\")\n",
    "        process_one_fold(run, device, results_queue, myGlobalDS, mainDir, num_epochs)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbf435b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:42.088889Z",
     "iopub.status.busy": "2025-02-26T12:10:42.088677Z",
     "iopub.status.idle": "2025-02-26T12:10:42.093872Z",
     "shell.execute_reply": "2025-02-26T12:10:42.092804Z"
    },
    "papermill": {
     "duration": 0.023187,
     "end_time": "2025-02-26T12:10:42.095540",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.072353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#from multiprocessing import Queue\\nimport torch.multiprocessing as mp\\n\\n\\nimport time\\nmainDir=\\'/kaggle/input/mynumpyfiles/NumpyDS\\'\\nnbFold=5\\nnum_epochs=60\\nmanager = mp.Manager()  # ✅ Crée un manager pour gérer des objets entre processus\\nresults_queue = manager.Queue()  # ✅ Création de la queue compatible avec `xmp.spawn()`\\n\\nxmp.spawn(_mp_fn, args=(results_queue, nbFold, myGlobalDs, mainDir, num_epochs), nprocs=xm.xrt_world_size(), start_method=\\'fork\\')\\n\\n# 📌 Collecte des résultats\\ntrain_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\\ntime.sleep(5) \\nwhile not results_queue.empty():\\n    t_loss, te_loss, t_acc, te_acc = results_queue.get()\\n    train_losses.append(t_loss)\\n    test_losses.append(te_loss)\\n    train_accuracies.append(t_acc)\\n    test_accuracies.append(te_acc)\\n\\n# 📌 Sauvegarde des résultats\\nnp.save(\"/kaggle/working/epoch_train_losses.npy\", np.array(train_losses))\\nnp.save(\"/kaggle/working/epoch_test_losses.npy\", np.array(test_losses))\\nnp.save(\"/kaggle/working/epoch_train_accuracies.npy\", np.array(train_accuracies))\\nnp.save(\"/kaggle/working/epoch_test_accuracies.npy\", np.array(test_accuracies))\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#from multiprocessing import Queue\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "import time\n",
    "mainDir='/kaggle/input/mynumpyfiles/NumpyDS'\n",
    "nbFold=5\n",
    "num_epochs=60\n",
    "manager = mp.Manager()  # ✅ Crée un manager pour gérer des objets entre processus\n",
    "results_queue = manager.Queue()  # ✅ Création de la queue compatible avec `xmp.spawn()`\n",
    "\n",
    "xmp.spawn(_mp_fn, args=(results_queue, nbFold, myGlobalDs, mainDir, num_epochs), nprocs=xm.xrt_world_size(), start_method='fork')\n",
    "\n",
    "# 📌 Collecte des résultats\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
    "time.sleep(5) \n",
    "while not results_queue.empty():\n",
    "    t_loss, te_loss, t_acc, te_acc = results_queue.get()\n",
    "    train_losses.append(t_loss)\n",
    "    test_losses.append(te_loss)\n",
    "    train_accuracies.append(t_acc)\n",
    "    test_accuracies.append(te_acc)\n",
    "\n",
    "# 📌 Sauvegarde des résultats\n",
    "np.save(\"/kaggle/working/epoch_train_losses.npy\", np.array(train_losses))\n",
    "np.save(\"/kaggle/working/epoch_test_losses.npy\", np.array(test_losses))\n",
    "np.save(\"/kaggle/working/epoch_train_accuracies.npy\", np.array(train_accuracies))\n",
    "np.save(\"/kaggle/working/epoch_test_accuracies.npy\", np.array(test_accuracies))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09ad7f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:42.126923Z",
     "iopub.status.busy": "2025-02-26T12:10:42.126684Z",
     "iopub.status.idle": "2025-02-26T12:10:42.132020Z",
     "shell.execute_reply": "2025-02-26T12:10:42.131081Z"
    },
    "papermill": {
     "duration": 0.023131,
     "end_time": "2025-02-26T12:10:42.133704",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.110573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_epochs=15\\nepochs=range(1, num_epochs+1)\\nplt.figure(figsize=(12, 5))\\n\\n# Courbe de la Loss\\nplt.subplot(1, 2, 1)\\nplt.plot(epochs, train_loss_mean, label=\"Train Loss\", color=\"blue\")\\nplt.plot(epochs, test_loss_mean, label=\"Test Loss\", color=\"orange\")\\n\\n\\nplt.xlabel(\"Epochs\")\\nplt.ylabel(\"Loss\")\\nplt.title(\"Learning Curve - Loss \")\\nplt.legend()\\n\\n# Courbe de l\\'Accuracy\\nplt.subplot(1, 2, 2)\\nplt.plot(epochs, train_accuracy_mean, label=\"Train Accuracy\", color=\"blue\")\\nplt.plot(epochs, test_accuracy_mean, label=\"Test Accuracy\", color=\"orange\")\\n\\nplt.xlabel(\"Epochs\")\\nplt.ylabel(\"Accuracy\")\\nplt.title(\"Learning Curve - Accuracy\")\\nplt.legend()\\n\\nplt.show()'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"num_epochs=15\n",
    "epochs=range(1, num_epochs+1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Courbe de la Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss_mean, label=\"Train Loss\", color=\"blue\")\n",
    "plt.plot(epochs, test_loss_mean, label=\"Test Loss\", color=\"orange\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve - Loss \")\n",
    "plt.legend()\n",
    "\n",
    "# Courbe de l'Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracy_mean, label=\"Train Accuracy\", color=\"blue\")\n",
    "plt.plot(epochs, test_accuracy_mean, label=\"Test Accuracy\", color=\"orange\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve - Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa9a5f9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:42.166021Z",
     "iopub.status.busy": "2025-02-26T12:10:42.165778Z",
     "iopub.status.idle": "2025-02-26T12:10:42.170956Z",
     "shell.execute_reply": "2025-02-26T12:10:42.169853Z"
    },
    "papermill": {
     "duration": 0.022907,
     "end_time": "2025-02-26T12:10:42.172136",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.149229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'epochs=range(1, num_epochs+1)\\nplt.figure(figsize=(12, 5))\\n\\n# Accuracy curve for train\\nplt.subplot(1, 2, 1)\\nplt.plot(epochs, train_accuracy_mean, label=\"Train Accuracy\", color=\"blue\")\\nplt.fill_between(epochs, train_accuracy_mean-train_accuracy_std,train_accuracy_mean+train_accuracy_std,alpha=0.2, color=\"blue\")\\n\\n\\nplt.xlabel(\"Epochs\")\\nplt.ylabel(\"Loss\")\\nplt.title(\"Learning Curve - Train Accuracy \")\\nplt.legend()\\n\\n# Courbe de l\\'Accuracy for Test\\nplt.subplot(1, 2, 2)\\nplt.plot(epochs, test_accuracy_mean, label=\"Test Accuracy\", color=\"orange\")\\nplt.fill_between(epochs, test_accuracy_mean-test_accuracy_std,test_accuracy_mean+test_accuracy_std,alpha=0.2, color=\"orange\")\\n\\nplt.xlabel(\"Epochs\")\\nplt.ylabel(\"Accuracy\")\\nplt.title(\"Learning Curve - Test Accuracy\")\\nplt.legend()\\n\\nplt.show()'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"epochs=range(1, num_epochs+1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy curve for train\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_accuracy_mean, label=\"Train Accuracy\", color=\"blue\")\n",
    "plt.fill_between(epochs, train_accuracy_mean-train_accuracy_std,train_accuracy_mean+train_accuracy_std,alpha=0.2, color=\"blue\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve - Train Accuracy \")\n",
    "plt.legend()\n",
    "\n",
    "# Courbe de l'Accuracy for Test\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, test_accuracy_mean, label=\"Test Accuracy\", color=\"orange\")\n",
    "plt.fill_between(epochs, test_accuracy_mean-test_accuracy_std,test_accuracy_mean+test_accuracy_std,alpha=0.2, color=\"orange\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve - Test Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed00aba",
   "metadata": {
    "papermill": {
     "duration": 0.014954,
     "end_time": "2025-02-26T12:10:42.202466",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.187512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Learning curve with respect to size of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bf73f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:42.234383Z",
     "iopub.status.busy": "2025-02-26T12:10:42.234174Z",
     "iopub.status.idle": "2025-02-26T12:10:42.240245Z",
     "shell.execute_reply": "2025-02-26T12:10:42.239504Z"
    },
    "papermill": {
     "duration": 0.024148,
     "end_time": "2025-02-26T12:10:42.241982",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.217834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nimport matplotlib.pyplot as plt\\nfrom torch.utils.data import Subset\\nimport torch_xla.distributed.parallel_loader as pl\\n\\ndef learning_curve_pytorch(model, device, full_train_dataset, train_dl, test_dl, mainDir, num_epochs=5):\\n    \\n    #Prepares a learning curve where accuracy is traced with respect to the size of the training dataset.\\n    \\n    #- model : CNN PyTorch model (ToolClassifier())\\n    #- full_train_dataset : Full training dataset before dataloader\\n    #- train_dl, val_dl : dataloaders\\n    #- num_epochs : number of epoch for each parts of the train dataset\\n    #- num_runs : number of times full training is repeated \\n    \\n    \\n    # Définir les proportions du dataset d\\'entraînement qu\\'on va tester\\n    train_sizes = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]  # 10%, 20% 30%, .., 100%\\n    #These lists are of size (, size of train_sizes)\\n    train_acc_run=[]\\n    train_acc_run=[]\\n    test_acc_run=[]\\n    train_los_run=[]\\n    test_los_run=[]\\n\\n    for frac in train_sizes:\\n        subset_size = int(len(full_train_dataset) * frac)\\n        subset_indices = np.random.choice(len(full_train_dataset), subset_size, replace=False)\\n        train_subset =  full_train_dataset.iloc[subset_indices].copy()\\n        ds_train=DataDS(train_subset, mainDir)\\n\\n        # Créer un DataLoader pour ce sous-ensemble\\n        train_subset_dl = DataLoader(ds_train, batch_size=8, shuffle=True)\\n        #With TPU\\n        train_subset_dl=pl.MpDeviceLoader(train_subset_dl, device)\\n        \\n\\n        xm.master_print(f\"\\n -Training with {subset_size} samples ({frac*100:.0f}%)\")\\n        \\n        # Train model on the subset training set \\n        model_copy = type(model)()  # Copy the model to start anew\\n        model_copy.to(device)\\n        \\n        #training with learning curve gives 4 lists the size of num_epochs we need to know only what happened at the end\\n        train_los, train_acc, test_los, test_acc = training_with_learning_curve(model_copy, device, train_subset_dl, test_dl, num_epochs)\\n\\n        #These lists are of size (,size of train_sizes)\\n        train_acc_run.append(train_acc[-1])  # Accuracy of the last epoch\\n        test_acc_run.append(test_acc[-1])\\n        train_los_run.append(train_los[-1]) #Loss of the last epoch\\n        test_los_run.append(test_los[-1])\\n    return train_acc_run, test_acc_run, train_los_run, test_los_run\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "def learning_curve_pytorch(model, device, full_train_dataset, train_dl, test_dl, mainDir, num_epochs=5):\n",
    "    \n",
    "    #Prepares a learning curve where accuracy is traced with respect to the size of the training dataset.\n",
    "    \n",
    "    #- model : CNN PyTorch model (ToolClassifier())\n",
    "    #- full_train_dataset : Full training dataset before dataloader\n",
    "    #- train_dl, val_dl : dataloaders\n",
    "    #- num_epochs : number of epoch for each parts of the train dataset\n",
    "    #- num_runs : number of times full training is repeated \n",
    "    \n",
    "    \n",
    "    # Définir les proportions du dataset d'entraînement qu'on va tester\n",
    "    train_sizes = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]  # 10%, 20% 30%, .., 100%\n",
    "    #These lists are of size (, size of train_sizes)\n",
    "    train_acc_run=[]\n",
    "    train_acc_run=[]\n",
    "    test_acc_run=[]\n",
    "    train_los_run=[]\n",
    "    test_los_run=[]\n",
    "\n",
    "    for frac in train_sizes:\n",
    "        subset_size = int(len(full_train_dataset) * frac)\n",
    "        subset_indices = np.random.choice(len(full_train_dataset), subset_size, replace=False)\n",
    "        train_subset =  full_train_dataset.iloc[subset_indices].copy()\n",
    "        ds_train=DataDS(train_subset, mainDir)\n",
    "\n",
    "        # Créer un DataLoader pour ce sous-ensemble\n",
    "        train_subset_dl = DataLoader(ds_train, batch_size=8, shuffle=True)\n",
    "        #With TPU\n",
    "        train_subset_dl=pl.MpDeviceLoader(train_subset_dl, device)\n",
    "        \n",
    "\n",
    "        xm.master_print(f\"\\n -Training with {subset_size} samples ({frac*100:.0f}%)\")\n",
    "        \n",
    "        # Train model on the subset training set \n",
    "        model_copy = type(model)()  # Copy the model to start anew\n",
    "        model_copy.to(device)\n",
    "        \n",
    "        #training with learning curve gives 4 lists the size of num_epochs we need to know only what happened at the end\n",
    "        train_los, train_acc, test_los, test_acc = training_with_learning_curve(model_copy, device, train_subset_dl, test_dl, num_epochs)\n",
    "\n",
    "        #These lists are of size (,size of train_sizes)\n",
    "        train_acc_run.append(train_acc[-1])  # Accuracy of the last epoch\n",
    "        test_acc_run.append(test_acc[-1])\n",
    "        train_los_run.append(train_los[-1]) #Loss of the last epoch\n",
    "        test_los_run.append(test_los[-1])\n",
    "    return train_acc_run, test_acc_run, train_los_run, test_los_run\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc36e2e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:42.274190Z",
     "iopub.status.busy": "2025-02-26T12:10:42.273962Z",
     "iopub.status.idle": "2025-02-26T12:10:42.281241Z",
     "shell.execute_reply": "2025-02-26T12:10:42.280274Z"
    },
    "papermill": {
     "duration": 0.024978,
     "end_time": "2025-02-26T12:10:42.282484",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.257506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nbFold = 5\\n\\nall_train_acc=[]\\nall_test_acc=[]\\nall_train_los=[]\\nall_test_los=[]\\n\\n\\n\\nfor fold in range(nbFold):\\n    \\n    xm.master_print(f\"Gathering data on train set size for fold : {fold+1}/{nbFold}\")\\n    myTrain=myGlobalDs[(myGlobalDs[\\'Fold\\']==fold) & (myGlobalDs[\\'Type\\']==\\'Train\\')].copy()\\n    myTrain=myTrain.drop(columns=[\\'Fold\\', \\'Type\\', \\'nb\\', \\'condLabel\\'])\\n        \\n    # trainFold0 is an instance of DataDS, constructor needs :\\n    #         A dataframe with at least relative paths in a column \\'FilePath\\' and a column \\'LabelIDs\\'\\n    #         An absolute path which is a level above the relative path mentioned in the dataframe \\n    \\n    #Feed train set to dataloader\\n    trainFoldi=DataDS(myTrain, mainDir)\\n    train_dataloader = DataLoader(trainFoldi, batch_size=8, shuffle=True)\\n    \\n    #Do the same loading for the test set\\n    myTest=myGlobalDs[(myGlobalDs[\\'Fold\\']==fold) & (myGlobalDs[\\'Type\\']==\\'Test\\')].copy()\\n    myTest=myTest.drop(columns=[\\'Fold\\', \\'Type\\', \\'nb\\', \\'condLabel\\'])\\n    \\n    #Feed test set to dataloader\\n    testFoldi=DataDS(myTest, mainDir)\\n    test_dataloader = DataLoader(testFoldi, batch_size=8, shuffle=True)\\n        \\n \\n    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\\n    #With TPU\\n    device = xm.xla_device()\\n    test_dataloader=pl.MpDeviceLoader(train_dataloader, device)\\n    train_dataloader=pl.MpDeviceLoader(test_dataloader, device)\\n    \\n    # Create the model and put it on the GPU if available\\n    myModel = ToolClassifier()\\n    \\n    myModel = myModel.to(device)\\n    # Check that it is on Cuda\\n    xm.master_print(f\"Model is on: {next(myModel.parameters()).device}\")\\n    \\n    \\n    num_epochs=8\\n    train_acc_run, test_acc_run, train_los_run, test_los_run=learning_curve_pytorch(myModel, device, myTrain, train_dataloader, test_dataloader, mainDir, num_epochs=num_epochs)\\n    all_train_acc.append(train_acc_run)\\n    all_test_acc.append(test_acc_run)\\n    all_train_los.append(train_los_run)\\n    all_test_los.append(test_los_run)\\n\\nall_train_acc=np.array(all_train_acc)\\nall_test_acc=np.array(all_test_acc)\\nall_train_los=np.array(all_train_los)\\nall_test_los=np.array(all_test_los)\\n\\n#Save to output\\nnp.save(\"/kaggle/working/size_train_losses.npy\", all_train_los)\\nnp.save(\"/kaggle/working/size_test_losses.npy\", all_test_los)\\nnp.save(\"/kaggle/working/size_train_accuracies.npy\", all_train_acc)\\nnp.save(\"/kaggle/working/size_test_accuracies.npy\", all_test_acc)\\n\\n#calculate mean and std for each train_sizes\\ntrain_acc_mean, train_acc_std=np.mean(all_train_acc, axis=0), np.std(all_train_acc, axis=0)\\ntest_acc_mean, test_acc_std=np.mean(all_test_acc, axis=0), np.std(all_test_acc, axis=0)\\ntrain_los_mean, train_los_std=np.mean(all_train_los, axis=0), np.std(all_train_los, axis=0)\\ntest_los_mean, test_los_std=np.mean(all_test_los, axis=0), np.std(all_test_los, axis=0)\\n\\n\\n# Trace Learning Curves with stds\\nx_values=np.array(train_sizes)*len(myTrain)\\nplt.figure(figsize=(12, 5))\\n# Courbe de l\\'Accuracy\\nplt.subplot(1, 2, 2)\\nplt.plot(x_values, train_acc_mean, label=\"Train Accuracy\", color=\"blue\")\\nplt.fill_between(x_values, train_acc_mean-train_acc_std,train_acc_mean+train_acc_std,alpha=0.2, color=\"blue\")\\n\\n\\nplt.plot(epochs, test_acc_mean, label=\"Test Loss\", color=\"orange\")\\nplt.fill_between(x_values, test_acc_mean-test_acc_std,test_acc_mean+test_acc_std,alpha=0.2, color=\"orange\")\\n\\nplt.xlabel(\"Training Set Size\")\\nplt.ylabel(\"Accuracy\")\\nplt.title(\"Learning Curve - Accuracy vs Training Set Size with Std Dev\")\\nplt.legend()\\n\\n\\n# Loss curve\\nplt.subplot(1, 2, 1)\\nplt.plot(x_values, train_los_mean, label=\"Train Loss\", color=\"blue\")\\nplt.fill_between(x_values, train_los_mean-train_los_std,train_los_mean+train_los_std,alpha=0.2, color=\"blue\")\\n\\n\\nplt.plot(x_values, test_los_mean, label=\"Test Loss\", color=\"orange\")\\nplt.fill_between(x_values, test_los_mean-test_los_std,test_los_mean+test_los_std,alpha=0.2, color=\"orange\")\\n\\nplt.xlabel(\"Training Set Size\")\\nplt.ylabel(\"Loss\")\\nplt.title(\"Learning Curve - Loss vs Training Set Size with Std Dev\")\\nplt.legend()\\n\\n                 \\nplt.show()'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"nbFold = 5\n",
    "\n",
    "all_train_acc=[]\n",
    "all_test_acc=[]\n",
    "all_train_los=[]\n",
    "all_test_los=[]\n",
    "\n",
    "\n",
    "\n",
    "for fold in range(nbFold):\n",
    "    \n",
    "    xm.master_print(f\"Gathering data on train set size for fold : {fold+1}/{nbFold}\")\n",
    "    myTrain=myGlobalDs[(myGlobalDs['Fold']==fold) & (myGlobalDs['Type']=='Train')].copy()\n",
    "    myTrain=myTrain.drop(columns=['Fold', 'Type', 'nb', 'condLabel'])\n",
    "        \n",
    "    # trainFold0 is an instance of DataDS, constructor needs :\n",
    "    #         A dataframe with at least relative paths in a column 'FilePath' and a column 'LabelIDs'\n",
    "    #         An absolute path which is a level above the relative path mentioned in the dataframe \n",
    "    \n",
    "    #Feed train set to dataloader\n",
    "    trainFoldi=DataDS(myTrain, mainDir)\n",
    "    train_dataloader = DataLoader(trainFoldi, batch_size=8, shuffle=True)\n",
    "    \n",
    "    #Do the same loading for the test set\n",
    "    myTest=myGlobalDs[(myGlobalDs['Fold']==fold) & (myGlobalDs['Type']=='Test')].copy()\n",
    "    myTest=myTest.drop(columns=['Fold', 'Type', 'nb', 'condLabel'])\n",
    "    \n",
    "    #Feed test set to dataloader\n",
    "    testFoldi=DataDS(myTest, mainDir)\n",
    "    test_dataloader = DataLoader(testFoldi, batch_size=8, shuffle=True)\n",
    "        \n",
    " \n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #With TPU\n",
    "    device = xm.xla_device()\n",
    "    test_dataloader=pl.MpDeviceLoader(train_dataloader, device)\n",
    "    train_dataloader=pl.MpDeviceLoader(test_dataloader, device)\n",
    "    \n",
    "    # Create the model and put it on the GPU if available\n",
    "    myModel = ToolClassifier()\n",
    "    \n",
    "    myModel = myModel.to(device)\n",
    "    # Check that it is on Cuda\n",
    "    xm.master_print(f\"Model is on: {next(myModel.parameters()).device}\")\n",
    "    \n",
    "    \n",
    "    num_epochs=8\n",
    "    train_acc_run, test_acc_run, train_los_run, test_los_run=learning_curve_pytorch(myModel, device, myTrain, train_dataloader, test_dataloader, mainDir, num_epochs=num_epochs)\n",
    "    all_train_acc.append(train_acc_run)\n",
    "    all_test_acc.append(test_acc_run)\n",
    "    all_train_los.append(train_los_run)\n",
    "    all_test_los.append(test_los_run)\n",
    "\n",
    "all_train_acc=np.array(all_train_acc)\n",
    "all_test_acc=np.array(all_test_acc)\n",
    "all_train_los=np.array(all_train_los)\n",
    "all_test_los=np.array(all_test_los)\n",
    "\n",
    "#Save to output\n",
    "np.save(\"/kaggle/working/size_train_losses.npy\", all_train_los)\n",
    "np.save(\"/kaggle/working/size_test_losses.npy\", all_test_los)\n",
    "np.save(\"/kaggle/working/size_train_accuracies.npy\", all_train_acc)\n",
    "np.save(\"/kaggle/working/size_test_accuracies.npy\", all_test_acc)\n",
    "\n",
    "#calculate mean and std for each train_sizes\n",
    "train_acc_mean, train_acc_std=np.mean(all_train_acc, axis=0), np.std(all_train_acc, axis=0)\n",
    "test_acc_mean, test_acc_std=np.mean(all_test_acc, axis=0), np.std(all_test_acc, axis=0)\n",
    "train_los_mean, train_los_std=np.mean(all_train_los, axis=0), np.std(all_train_los, axis=0)\n",
    "test_los_mean, test_los_std=np.mean(all_test_los, axis=0), np.std(all_test_los, axis=0)\n",
    "\n",
    "\n",
    "# Trace Learning Curves with stds\n",
    "x_values=np.array(train_sizes)*len(myTrain)\n",
    "plt.figure(figsize=(12, 5))\n",
    "# Courbe de l'Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_values, train_acc_mean, label=\"Train Accuracy\", color=\"blue\")\n",
    "plt.fill_between(x_values, train_acc_mean-train_acc_std,train_acc_mean+train_acc_std,alpha=0.2, color=\"blue\")\n",
    "\n",
    "\n",
    "plt.plot(epochs, test_acc_mean, label=\"Test Loss\", color=\"orange\")\n",
    "plt.fill_between(x_values, test_acc_mean-test_acc_std,test_acc_mean+test_acc_std,alpha=0.2, color=\"orange\")\n",
    "\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve - Accuracy vs Training Set Size with Std Dev\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Loss curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_values, train_los_mean, label=\"Train Loss\", color=\"blue\")\n",
    "plt.fill_between(x_values, train_los_mean-train_los_std,train_los_mean+train_los_std,alpha=0.2, color=\"blue\")\n",
    "\n",
    "\n",
    "plt.plot(x_values, test_los_mean, label=\"Test Loss\", color=\"orange\")\n",
    "plt.fill_between(x_values, test_los_mean-test_los_std,test_los_mean+test_los_std,alpha=0.2, color=\"orange\")\n",
    "\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve - Loss vs Training Set Size with Std Dev\")\n",
    "plt.legend()\n",
    "\n",
    "                 \n",
    "plt.show()\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f668922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:42.314861Z",
     "iopub.status.busy": "2025-02-26T12:10:42.314610Z",
     "iopub.status.idle": "2025-02-26T12:10:42.321638Z",
     "shell.execute_reply": "2025-02-26T12:10:42.320547Z"
    },
    "papermill": {
     "duration": 0.024572,
     "end_time": "2025-02-26T12:10:42.322648",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.298076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def process_train_sizes(run, device, results_queue, myGlobalDs, mainDir, num_epochs, train_size):\\n# 📌 Chargement des données\\n    myTrain = myGlobalDs[(myGlobalDs[\\'Fold\\'] == run) & (myGlobalDs[\\'Type\\'] == \\'Train\\')].copy()\\n    myTrain = myTrain.drop(columns=[\\'Fold\\', \\'Type\\', \\'nb\\', \\'condLabel\\'])\\n    subset_size = int(len(myTrain) * train_size)\\n    subset_indices = np.random.choice(len(myTrain), subset_size, replace=False)\\n    train_subset =  myTrain.iloc[subset_indices].copy()\\n    trainFoldi = DataDS(train_subset, mainDir)\\n\\n    myTest = myGlobalDs[(myGlobalDs[\\'Fold\\'] == run) & (myGlobalDs[\\'Type\\'] == \\'Test\\')].copy()\\n    myTest = myTest.drop(columns=[\\'Fold\\', \\'Type\\', \\'nb\\', \\'condLabel\\'])\\n    testFoldi = DataDS(myTest, mainDir)\\n\\n\\n    # Select subset to be trained\\n\\n\\n    # Distribute datasets on 8 TPU Cores\\n    #train_sampler = DistributedSampler(trainFoldi, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=True)\\n    #test_sampler = DistributedSampler(testFoldi, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=False)\\n\\n    # Create the DataLoaders\\n    train_dataloader = DataLoader(trainFoldi, batch_size=8, shuffle=True)\\n    test_dataloader = DataLoader(testFoldi, batch_size=8, shuffle=False)\\n    \\n    # Create the model to be trained\\n    myModel = ToolClassifier().to(device)\\n    xm.broadcast_master_param(myModel)\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = optim.Adam(myModel.parameters(), lr=0.001)\\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_dataloader), epochs=num_epochs, anneal_strategy=\\'linear\\')\\n\\n    # Get global train mean an dstd for normalization\\n    trainMean, trainStd = normalize_ds(train_dataloader, device)\\n    \\n    # List to store results\\n    train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\\n\\n    # train -> test -> train loop for each epoch\\n    #training_one_epoch(model, optimizer, criterion, scheduler, device, train_dl, trainMean, trainStd, epochNb)\\n    #testing_one_epoch(model, criterion, device, test_dl, trainMean, trainStd, epochNb)\\n    for epoch in range(num_epochs):\\n        train_loss, train_acc = training_one_epoch(myModel, optimizer, criterion, scheduler, device, train_dataloader, trainMean, trainStd, epoch)\\n        test_loss, test_acc = testing_one_epoch(myModel, criterion, device, test_dataloader, trainMean, trainStd, epoch)\\n\\n        train_losses.append(train_loss)\\n        test_losses.append(test_loss)\\n        train_accuracies.append(train_acc)\\n        test_accuracies.append(test_acc)\\n\\n        #torch.cuda.empty_cache()  # Libère la mémoire GPU si applicable\\n        gc.collect()  # Libère la mémoire du CPU\\n        xm.mark_step()  # Libère la mémoire TPU\\n\\n    # A TPU sync at the end of an epoch \\n    xm.rendezvous(\"epoch_sync\")\\n    xm.master_print(f\"Training finished for Fold {run} - size of train set is : {train_size*100:.1f}% ({len(train_subset)} samples) \")\\n    \\n    # Send results to main process\\n    results_queue.put((train_size, train_losses, test_losses, train_accuracies, test_accuracies))\\n    \\n    # Cleanup of TPU memory\\n    del myModel\\n    gc.collect()\\n    xm.mark_step()  \\n    xm.master_print(f\"Cleanup over for TPU Core {xm.get_ordinal()}\")\\n    xm.rendezvous(\"cleanup\")'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def process_train_sizes(run, device, results_queue, myGlobalDs, mainDir, num_epochs, train_size):\n",
    "# 📌 Chargement des données\n",
    "    myTrain = myGlobalDs[(myGlobalDs['Fold'] == run) & (myGlobalDs['Type'] == 'Train')].copy()\n",
    "    myTrain = myTrain.drop(columns=['Fold', 'Type', 'nb', 'condLabel'])\n",
    "    subset_size = int(len(myTrain) * train_size)\n",
    "    subset_indices = np.random.choice(len(myTrain), subset_size, replace=False)\n",
    "    train_subset =  myTrain.iloc[subset_indices].copy()\n",
    "    trainFoldi = DataDS(train_subset, mainDir)\n",
    "\n",
    "    myTest = myGlobalDs[(myGlobalDs['Fold'] == run) & (myGlobalDs['Type'] == 'Test')].copy()\n",
    "    myTest = myTest.drop(columns=['Fold', 'Type', 'nb', 'condLabel'])\n",
    "    testFoldi = DataDS(myTest, mainDir)\n",
    "\n",
    "\n",
    "    # Select subset to be trained\n",
    "\n",
    "\n",
    "    # Distribute datasets on 8 TPU Cores\n",
    "    #train_sampler = DistributedSampler(trainFoldi, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=True)\n",
    "    #test_sampler = DistributedSampler(testFoldi, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=False)\n",
    "\n",
    "    # Create the DataLoaders\n",
    "    train_dataloader = DataLoader(trainFoldi, batch_size=8, shuffle=True)\n",
    "    test_dataloader = DataLoader(testFoldi, batch_size=8, shuffle=False)\n",
    "    \n",
    "    # Create the model to be trained\n",
    "    myModel = ToolClassifier().to(device)\n",
    "    xm.broadcast_master_param(myModel)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(myModel.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_dataloader), epochs=num_epochs, anneal_strategy='linear')\n",
    "\n",
    "    # Get global train mean an dstd for normalization\n",
    "    trainMean, trainStd = normalize_ds(train_dataloader, device)\n",
    "    \n",
    "    # List to store results\n",
    "    train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
    "\n",
    "    # train -> test -> train loop for each epoch\n",
    "    #training_one_epoch(model, optimizer, criterion, scheduler, device, train_dl, trainMean, trainStd, epochNb)\n",
    "    #testing_one_epoch(model, criterion, device, test_dl, trainMean, trainStd, epochNb)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = training_one_epoch(myModel, optimizer, criterion, scheduler, device, train_dataloader, trainMean, trainStd, epoch)\n",
    "        test_loss, test_acc = testing_one_epoch(myModel, criterion, device, test_dataloader, trainMean, trainStd, epoch)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        #torch.cuda.empty_cache()  # Libère la mémoire GPU si applicable\n",
    "        gc.collect()  # Libère la mémoire du CPU\n",
    "        xm.mark_step()  # Libère la mémoire TPU\n",
    "\n",
    "    # A TPU sync at the end of an epoch \n",
    "    xm.rendezvous(\"epoch_sync\")\n",
    "    xm.master_print(f\"Training finished for Fold {run} - size of train set is : {train_size*100:.1f}% ({len(train_subset)} samples) \")\n",
    "    \n",
    "    # Send results to main process\n",
    "    results_queue.put((train_size, train_losses, test_losses, train_accuracies, test_accuracies))\n",
    "    \n",
    "    # Cleanup of TPU memory\n",
    "    del myModel\n",
    "    gc.collect()\n",
    "    xm.mark_step()  \n",
    "    xm.master_print(f\"Cleanup over for TPU Core {xm.get_ordinal()}\")\n",
    "    xm.rendezvous(\"cleanup\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b60c3c8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:42.355423Z",
     "iopub.status.busy": "2025-02-26T12:10:42.355181Z",
     "iopub.status.idle": "2025-02-26T12:10:42.359912Z",
     "shell.execute_reply": "2025-02-26T12:10:42.358998Z"
    },
    "papermill": {
     "duration": 0.023223,
     "end_time": "2025-02-26T12:10:42.361679",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.338456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def _mp_fn(index, results_queue, nbFold, myGlobalDS, mainDir, num_epochs, sizes):\\n    device = xm.xla_device()\\n\\n    \\n    for run in range(nbFold):\\n        for s in sizes:\\n            xm.master_print(f\"\\n Run {run+1}/{nbFold} sur TPU Core {xm.get_ordinal()} for size : {s}...\\n\")\\n            process_train_sizes(run, device, results_queue, myGlobalDS, mainDir, num_epochs, s)\\n            xm.rendezvous(\"sync_after_fold\")\\n    xm.master_print(f\"TPU Core {xm.get_ordinal()} is done with all its tasks\")'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def _mp_fn(index, results_queue, nbFold, myGlobalDS, mainDir, num_epochs, sizes):\n",
    "    device = xm.xla_device()\n",
    "\n",
    "    \n",
    "    for run in range(nbFold):\n",
    "        for s in sizes:\n",
    "            xm.master_print(f\"\\n Run {run+1}/{nbFold} sur TPU Core {xm.get_ordinal()} for size : {s}...\\n\")\n",
    "            process_train_sizes(run, device, results_queue, myGlobalDS, mainDir, num_epochs, s)\n",
    "            xm.rendezvous(\"sync_after_fold\")\n",
    "    xm.master_print(f\"TPU Core {xm.get_ordinal()} is done with all its tasks\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961112d",
   "metadata": {
    "papermill": {
     "duration": 0.015801,
     "end_time": "2025-02-26T12:10:42.393628",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.377827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893daf12",
   "metadata": {
    "papermill": {
     "duration": 0.015822,
     "end_time": "2025-02-26T12:10:42.425368",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.409546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2a93680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:10:42.458631Z",
     "iopub.status.busy": "2025-02-26T12:10:42.458390Z",
     "iopub.status.idle": "2025-02-26T12:10:42.464480Z",
     "shell.execute_reply": "2025-02-26T12:10:42.463402Z"
    },
    "papermill": {
     "duration": 0.024704,
     "end_time": "2025-02-26T12:10:42.465856",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.441152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch.multiprocessing as mp\\nimport pickle\\nimport time\\nimport torch_xla._internal.pjrt as pjrt\\n\\nos.environ[\\'PJRT_DEVICE\\'] = \\'TPU\\'\\nos.environ[\\'XRT_TPU_CONFIG\\'] = \\'localservice;0;localhost:51011\\'\\n\\n#pjrt.  # Arrête les processus TPU en cours\\n\\nmainDir=\\'/kaggle/input/mynumpyfiles/NumpyDS\\'\\nnbFold=5\\nnum_epochs=18\\n\\nmanager = mp.Manager()  # ✅ Crée un manager pour gérer des objets entre processus\\nresults_queue = manager.Queue()  # ✅ Création de la queue compatible avec `xmp.spawn()`\\n\\ntrain_sizes=[0.1, 0.3, 0.5, 0.7, 1]\\nnprocs = min(8, xr.global_runtime_device_count())\\n\\nxmp.spawn(_mp_fn, args=(results_queue, nbFold, myGlobalDs, mainDir, num_epochs, train_sizes), nprocs=1, start_method=\\'fork\\')\\n\\n# Collect the results in dictionnaries\\ntrain_losses_dict = {s: [] for s in train_sizes}\\ntest_losses_dict = {s: [] for s in train_sizes}\\ntrain_accuracies_dict = {s: [] for s in train_sizes}\\ntest_accuracies_dict = {s: [] for s in train_sizes}\\n\\n#initialize counter for timeout\\nstart=time.time()\\ntimeout=600\\nwhile results_queue.qsize() < nbFold * len(train_sizes):\\n    if time.time()-start > timeout:\\n        raise TimeoutError(\"TPUs take too much time to finish ! Verify the logs\")\\n    time.sleep(1)  # Wait for all tasks to be finished\\n\\nwhile not results_queue.empty():\\n    size, t_loss, te_loss, t_acc, te_acc = results_queue.get()  # ✅ On récupère directement `size`\\n    train_losses_dict[size].append(t_loss)\\n    test_losses_dict[size].append(te_loss)\\n    train_accuracies_dict[size].append(t_acc)\\n    test_accuracies_dict[size].append(te_acc)\\n\\n\\n# Store data (serialize)\\nwith open(\\'/kaggle/working/sizes_train_losses.pickle\\', \\'wb\\') as handle:\\n    pickle.dump(train_losses_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nwith open(\\'/kaggle/working/sizes_test_losses.pickle\\', \\'wb\\') as handle:\\n    pickle.dump(test_losses_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nwith open(\\'/kaggle/working/sizes_train_accuracies.pickle\\', \\'wb\\') as handle:\\n    pickle.dump(train_accuracies_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nwith open(\\'/kaggle/working/sizes_test_accuracies.pickle\\', \\'wb\\') as handle:\\n    pickle.dump(test_accuracies_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import torch.multiprocessing as mp\n",
    "import pickle\n",
    "import time\n",
    "import torch_xla._internal.pjrt as pjrt\n",
    "\n",
    "os.environ['PJRT_DEVICE'] = 'TPU'\n",
    "os.environ['XRT_TPU_CONFIG'] = 'localservice;0;localhost:51011'\n",
    "\n",
    "#pjrt.  # Arrête les processus TPU en cours\n",
    "\n",
    "mainDir='/kaggle/input/mynumpyfiles/NumpyDS'\n",
    "nbFold=5\n",
    "num_epochs=18\n",
    "\n",
    "manager = mp.Manager()  # ✅ Crée un manager pour gérer des objets entre processus\n",
    "results_queue = manager.Queue()  # ✅ Création de la queue compatible avec `xmp.spawn()`\n",
    "\n",
    "train_sizes=[0.1, 0.3, 0.5, 0.7, 1]\n",
    "nprocs = min(8, xr.global_runtime_device_count())\n",
    "\n",
    "xmp.spawn(_mp_fn, args=(results_queue, nbFold, myGlobalDs, mainDir, num_epochs, train_sizes), nprocs=1, start_method='fork')\n",
    "\n",
    "# Collect the results in dictionnaries\n",
    "train_losses_dict = {s: [] for s in train_sizes}\n",
    "test_losses_dict = {s: [] for s in train_sizes}\n",
    "train_accuracies_dict = {s: [] for s in train_sizes}\n",
    "test_accuracies_dict = {s: [] for s in train_sizes}\n",
    "\n",
    "#initialize counter for timeout\n",
    "start=time.time()\n",
    "timeout=600\n",
    "while results_queue.qsize() < nbFold * len(train_sizes):\n",
    "    if time.time()-start > timeout:\n",
    "        raise TimeoutError(\"TPUs take too much time to finish ! Verify the logs\")\n",
    "    time.sleep(1)  # Wait for all tasks to be finished\n",
    "\n",
    "while not results_queue.empty():\n",
    "    size, t_loss, te_loss, t_acc, te_acc = results_queue.get()  # ✅ On récupère directement `size`\n",
    "    train_losses_dict[size].append(t_loss)\n",
    "    test_losses_dict[size].append(te_loss)\n",
    "    train_accuracies_dict[size].append(t_acc)\n",
    "    test_accuracies_dict[size].append(te_acc)\n",
    "\n",
    "\n",
    "# Store data (serialize)\n",
    "with open('/kaggle/working/sizes_train_losses.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_losses_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('/kaggle/working/sizes_test_losses.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_losses_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('/kaggle/working/sizes_train_accuracies.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_accuracies_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('/kaggle/working/sizes_test_accuracies.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_accuracies_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc096a0",
   "metadata": {
    "papermill": {
     "duration": 0.016141,
     "end_time": "2025-02-26T12:10:42.497927",
     "exception": false,
     "start_time": "2025-02-26T12:10:42.481786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 6624622,
     "sourceId": 10691564,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8770.752336,
   "end_time": "2025-02-26T12:10:47.504131",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-26T09:44:36.751795",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
